Введение

SQL  (Structured Query Language  — «язык структурированных запросов») — декларативный язык программирования, применяемый для создания, модификации и управления данными в реляционной базе данных, управляемой соответствующей системой
управления базами данных. В нем отметим три ключевых момента: SQL – это ДЕКЛАРАТИВНЫЙ язык, который используется ДЛЯ УПРАВЛЕНИЯ ДАННЫМИ в РЕЛЯЦИОННЫХ базах. Давайте чуть подробнее посмотрим, что это значит…

1) Декларативный

Императивные ЯП, к которым относится большая часть тех, которые вы знаете, с которыми вам приходилось сталкиваться, заставляют вас четко описать "как" достичь нужного результата: булочку разрезать пополам, положить на нее котлету, дальше
соус, сыр, … и на выходе у вас, наверное, получится гамбургер, если вы нигде в процессе не ошиблись. В отличие от них декларативные ЯП (которых вам, наверное, встречалось всего два, зато они общеизвестны: HTML и SQL), позволяют просто 
описать "что" мы хотим получить. Например, просто подъехать к окну заказа и сказать "Хочу гамбургер!" И в этот момент нас не интересует ни при какой температуре, ни сколько времени он будет готовиться, ни кто конкретно его будет 
готовить - вся эта информация нам неинтересна, мы просто заказываем конкретный результат. Вообщем, декларативный означает что такой язык описывает что должно быть сделано, а не как это сделать (SQL, HTML, Haskell).

Безусловно, у обоих подходов есть плюсы и минусы – например, императивные языки позволяют лучше оперировать машинными ресурсами на "низовом" уровне: мы можем "отрезать" себе кусочек памяти, загрузить процессор конкретной задачей, зато 
приходится сильно постараться, чтобы где-нибудь не проехать по "чужой" памяти или не "споткнуться" о Null Pointer Exception.

Декларативные языки, как правило, позволяют вам существенно короче описать то, что вы хотите получить. Но, к сожалению, только в той области, под которую они "заточены". Например, упоминавшийся HTML хорошо приспособлен под отображение 
данных (в основном, текстовых), а это означает, что все остальное на нем делать будет либо вовсе невозможно, либо очень сложно и некомфортно.

А вот SQL "заточен" под...

2) Управление данными

Большинство императивных языков программирования взаимодействуют с какими-то АТОМАРНЫМИ, "штучными" вещами: состояниями системы или событиями, которые в ней происходят. В отличие от них, SQL работает с БОЛЬШИМИ наборами данных - 
записей или "строк". Строки группируются в таблицы, которые, будучи связаны некоторыми ОТНОШЕНИЯМИ между собой, образуют базу данных.

строка -> таблица -> база

3) Реляционные базы

Слово "отношения" здесь ключевое, поскольку именно оно определяет, что ваша СУБД является реляционной - то есть в такой базе будут находиться связанные таблицы. Потому что существуют достаточно много видов нереляционных СУБД, 
преимущество которых заключается в возможности, хоть и жертвуя универсальностью, под каждую конкретную прикладную задачу подобрать наиболее подходящий из них: Key-Value, документарные, графовые, поисковые ("заточенные" под 
полнотекстовый или фразовый поиск) или даже мультипарадигмальные, приближающиеся по возможностям к традиционным SQL-базам. Все эти варианты нереляционных СУБД никак не заставляют нас конкретизировать структуру хранения данных в нашей 
базе, и прямо рядом с ключом-числом можно положить ключ-строку или вовсе динамически заменить скалярное значение на список.

В отличие от них, в реляционных базах, структура жестко задается на моменте разработки, и ее нельзя быстро "перетряхнуть" в динамике - это достаточно сложный процесс. Безусловным стандартом работы с ними сейчас является именно SQL, 
универсальный по своим возможностям, его поддерживают все ведущие enterprise-СУБД. Хотя, конечно, не обошлось и без "ложки дегтя". Как и любой язык, SQL выработал со временем определенные "диалекты", и каждая СУБД старается "отрастить" 
свой, чтобы сделать использование именно своих особенностей еще удобнее. Поэтому, если у вас стоит задача писать максимально переносимый между СУБД софт, который будет все запросы формировать одинаково понятными для всех СУБД, то либо 
это будет очень сложным процессом, либо вы получите крайне неэффективные запросы, не использующие хоть какую-то специфику возможностей конкретной базы. То есть любой универсальный запрос на SQL будет одинаково выполняться на всех таких 
базах, но на всех - не настолько эффективно, насколько можно было бы сделать с учетом специфики.


###################
Хранение данных в реляционных базах

Реляционная БД:
- Заранее установленные связи
- Таблицы из строк и столбцов
- Столбец - тип атрибута
- Строка - атрибуты одного объекта
- Ячейка (поле записи) - значение



			                                      /-----------------------\
					                      | Столбец-атрибут       |
					                      |                       |  
	/-----------------------------------------------------|-----------------------|-----\
	|  Таблица			                      |	                      |     |
	|				                      |                       |     |
	|  /-----------\       /--------------------\         |  /------------------\ |     |
	|  |           |       |                    |         |  |                  | |     |
	|  \-----------/       \--------------------/         |  \------------------/ |     |
	|                                                     |                       |     |
	|  /-----------\       /--------------------\         |  /------------------\ |     |
	|  |           |       |                    |         |  |                  | |     |
        |  \-----------/       \--------------------/         |  \------------------/ |     |
        |                                                     |                       |     |
 /------|-----------------------------------------------------|---------------------- |-----|--------\                             
 |      |                             строка-объект           |                       |     |        |
 |      |   /-----------\       /--------------------\        |  /------------------\ |     |        |
 |      |   |           |       |                    |        |  |                  | |     |        |
 |      |   \-----------/       \--------------------/        |  \------------------/ |     |        |
 |      |                                                     |                       |     |        |
 \------|-----------------------------------------------------|-----------------------|-----|--------/
        |                                                     |                       |     |
        \-----------------------------------------------------|-----------------------|-----/            
                                                              |                       |
                                                              \-----------------------/




Все базы нужны для того, чтобы хранить какие-то прикладные данные, и в SQL-ориентированных некоторый класс объектов - например, "документы" (бумажка с подписью), с атрибутами "номер" и "дата" - будет представлен отдельной таблицей. 
Атрибуты объекта будут являться ее столбцами, экземпляры объектов - строками, а на пересечении - в поле конкретной строки - будет храниться значение данного атрибута для конкретного экземпляра (Номер = 123, Дата = 01.01.2000). Поэтому 
все строки одной таблицы имеют один и тот же формат, в отличие от нереляционных баз.


Между собой таблицы связываются какими-то отношениями, которые определяются ключами. Как правило, у любой таблицы есть первичный ключ (Primary Key, PK), и он необходим, чтобы уникально идентифицировать любую из строк этой таблицы.


Реляционная БД:
- первичный ключ в таблице (Primary Key, PK)
- внешние ключи на другие таблицы (Foreign Keys, FK)
- уникальные не-первичные ключи (Unique Keys)




	/--------------------------\                       /------------------------\
	|            Customers     |   	                   |      Products          |
	|--------------------------| 	                   |------------------------|
	| customerID  |  Int       | ----------\ 	   | productID | Int        |--------\
	| firstName   | String     |           |           | category  | String     |        |
	| lastName    | String     |           |           | price     | Money      |        |
	| birthDate   | Date       |           |           \------------------------/        |
	| moneySpent  | Money      |           |                                             |
	| annuversary | Date       |           |                                             |
 	\--------------------------/           |                                             |
                                               |                                             |
	/-------------------------\            |         /--------------------------\        |
	|          Employees      |            | 	 |         Orders           |        |
	|-------------------------|            |         |--------------------------|        | 
	| employeeID  | Int       | ----\      |   	 | orderID     | Int        |        |
	| firstName   | String    |     |      \-------> | customerID  | Int        |        |
	| lastName    | String    |     \--------------> | employeeID  | Int        |        |
	| birthDate   | Date      |		         | productID   | Int        | <------/
	\-------------|-----------/		         | orderTotal  | Money      |
					                 | orderDate   | Date       |
					                 \--------------------------/



Конечно, бывают ситуации, когда у вас в таблице по вполне определенным прикладными причинам может не быть первичного ключа. Например, вы пишете какую-то систему хранения логов, и запись данных в нее происходит настолько часто, что даже 
1 секунда не является уникальным идентификатором - когда у вас таких событий происходит за секунду 10, вы их все пишете, а время отличить между ними никак не можете. И тогда вам или придется добавить в таблицу в качестве первичного 
суррогатный ключ, или вообще стоит его не делать - если отдельные записи не требуется уникально идентифицировать, а на нее никто не ссылается. (Суррогатный ключ - дополнительное служебное поле, добавленное к уже имеющимся полям таблицы,
единственное предназначение которого - служить первичным ключом. Значение этого поля генерируется искусственно. Главное его достоинство - неизменность, он практически никогда не изменяется.) 

Потому что первичные ключи, классически, используются именно для того, чтобы иметь возможность сослаться на конкретную запись или провзаимодействовать с ней. А как раз чтобы "сослаться" со стороны подчиненной таблицы используются внешние 
ключи (Foreign Keys, FK) - они определяют по соответствию значений каких полей в дочерней и родительской таблице устанавливается связь. В принципе, внешний ключ может ссылаться не обязательно на первичный, но и на любой уникальный ключ 
(Unique Key), которых на таблице, в отличие от первичного, у вас может быть несколько. Например, в таблице, куда вы записываете всех своих клиентов, первичным ключом может выступать суррогатный идентификатор, а дополнительным уникальным
ключом - его ИНН.


###################
Особенности PostgreSQL

Пока мы все говорили про SQL в целом, давайте теперь коснемся особенностей непосредственно PostgreSQL. 

Во-первых, в отличие от некоторых других СУБД, PostgreSQL исповедует клиент-серверную архитектуру. Это означает, что у нас всегда есть некоторый клиент, который формирует запрос и по собственному протоколу "поверх" TCP/IP отправляет его
серверу. Как правило, этот запрос текстовый и содержит какие-то SQL-команды. А в ответ мы получаем некоторый код результата и, возможно, выборку. Для того, чтобы иметь возможность послать это все серверу и получить обратно, необходимо 
либо клиентское приложение, либо какая-то библиотека, необходимая для вашего приложения.

Клиент-серверная архитектура:
- запрос от клиента
- ответ от сервера
- собственный протокол over TCP-IP
- нужно клиентское приложение (или библиотека)


Общение с PostgreSQL-базой
Если вы будете развиваться в сфере работы с PostgreSQL, то рано или поздно станете хардкорным разработчиком или админом, или DBA, вы точно будете пользоваться утилитой psql - это нативный консольный клиент, который входит в состав 
стандартного дистрибутива самого PostgreSQL-сервера, поэтому "есть везде". Но пока вы не достигли таких вершин или просто не любите консоль, можете использовать любую из пары десятков GUI-утилит: pgAdmin, DBeaver, Navicat.

Если же вам необходимо интегрировать работу с PostgreSQL в свое приложение, то вам необходимо найти подходящую к вашему языку программирования клиентскую библиотеку - их более 30 вариантов, включая экзотические в наших широтах Haskel, 
Erlang или Rust. Даже офисные продукты вроде Access, Excel или даже 1C можно заставить работать как PostgreSQL-клиента, если использовать ODBC-драйвер. Так что возможности как-то повзаимодействовать с PostgreSQL ограничены исключительно
вашей фантазией. 

Библиотеки:
- libpq (C)
- libpqxx (C++)
- psycopg (Python)
- node-postgres (JavaScript)




______________________________________________________________________________________
Базовые SQL-команды. Базовый синтаксис и базовые типы данных


Так из чего же будет состоять ваша жизнь, когда вы будете активно писать на SQL? На 99% - это будет команда SELECT. А вместе все операторы, которые приведены на этом слайде, покрывают 99.9% всех типовых задач. То есть 99% - SELECT, 
а 0.9% - всякие CREATE, ALTER, INSERT, ... Потому что самая основная задача любой базы данных - это не столько хранение данных или их прием, сколько отдача.

SELECT - Получить выборку записей

CREATE - Создать объект СУБД
ALTER - Изменить объект СУБД
DROP - Удалить объект СУБД

INSERT - Вставить запись таблицы
UPDATE - Изменить запись таблицы
DELETE / TRUNCATE - Удалить запись таблицы

Правильно попросить ее "отдать", что мы хотим, чтобы она сделала это эффективно, не перегрузилась при этом, и вернула именно те данные, которые были нам нужны - это мы и постараемся научиться делать в рамках данного курса.


###################
Создаем демо-базу

Начнем с создания простейшей базы. А для того, чтобы что-то создать в SQL, нам необходима команда CREATE - она отвечает за создание в SQL любого типа объектов, но нас пока будут интересовать только две: CREATE DATABASE, чтобы создать 
базу, и CREATE TABLE, чтобы создать таблицу в ней:

CREATE DATABASE tst;

CREATE TABLE tbl(
  k         -- имя поля
    integer -- тип поля
, v
    text
);

При создании таблицы нам надо заранее определить имена и типы полей – то есть формат записей, которые там будут храниться – прямо в соответствии с определением реляционной базы данных.

Для удаления таблицы используем команду DROP TABLE имя_таблицы; Данная команда необратима, она удалит таблицу и также связанные с таблицей объекты.


###################
Базовый синтаксис

Если вдруг кто-то не догадался, то два минуса в предыдущем примере означают однострочный комментарий, прямо как "две косых" в привычных языках программирования вроде C или JavaScript, а многострочный так и вовсе выглядит точно так же.

-- это однострочный комментарий
/* а это -
         - многострочный */


И раз уж мы затронули тему синтаксиса, то тем, кто работал с другими СУБД некоторые вещи могут быть непривычны.  Во-первых, все поля регистронезависимы – как бы и где бы вы его не написали, оно будет приведено к нижнему регистру. Если 
же вам хочется указать для поля «странное» имя – например, с пробелом, по-русски, по-китайски, или просто сохранить его регистр, то указывать его надо везде в обычных двойных кавычках – никаких странностей вроде квадратных скобок или 
обратных апострофов. Зато, во-вторых, обычные строковые литералы бывают как в апострофах, так и в виде эскейп-последовательностей или даже «$-quoting-string».

fld      -- это поле/столбец
Fld      -- это то же самое поле
FLD      -- ... и это – все оно же (приводится к lower case)

"Fld"    -- а вот это – тоже поле, но совсем другое (кавычки дают регистрозависимость)

'str'    -- это строка

'st''r'  -- это строка с одинарным апострофом
E'st\'r' -- ... и это – она же
$$st'r$$ -- ... и даже вот это
$abcd$st'r$abcd$



###################
Базовые типы данных

При создании таблицы выше мы указывали имена полей и их типы. Какими могут быть имена, мы уже обсудили, давайте посмотрим теперь на то, какими могут быть их типы. Все типы в PostgreSQL можно разделить на базовые (числовые, символьные, 
даты/времени и логический тип) и расширенные.

1) Числовые типы
Числовые типы в PostgreSQL определяются своей разрядностью: 2-, 4- и 8-байтные целочисленные, 4- и 8-байтовые с переменной точностью (с плавающей точкой) и numeric/decimal с указанной точностью (хранится посимвольно).

           Имя      |     Размер     |                      Описание                            |                             Диапазон                                   |
--------------------|----------------|----------------------------------------------------------|------------------------------------------------------------------------| 
       smallint     |     2 байта    | целое в небольшом диапазоне                              | -32768 .. +32767                                                       |
--------------------|----------------|----------------------------------------------------------|------------------------------------------------------------------------|
        integer     |     4 байта    | типичный выбор для целых чисел                           | -2147483648 .. +2147483647                                             |
--------------------|----------------|----------------------------------------------------------|------------------------------------------------------------------------|
         bigint     |      8 байт    | целое в большом диапазоне                                | -9223372036854775808 .. +9223372036854775807                           |
--------------------|----------------|----------------------------------------------------------|------------------------------------------------------------------------|
       decimal      |   переменный   | вещественное число с указанной точностью                 | до 131072 цифр до десятичной точки и до 16383 - после                  |
--------------------|----------------|----------------------------------------------------------|------------------------------------------------------------------------|
       numeric      |   переменный   | вещественное число с указанной точностью                 | до 131072 цифр до десятичной точки и до 16383 - после                  |
--------------------|----------------|----------------------------------------------------------|------------------------------------------------------------------------|
          real      |    4 байта     | вещественное число с переменной точностью                | точность в пределах 6 десятичных цифр                                  |
--------------------|----------------|----------------------------------------------------------|------------------------------------------------------------------------|
double precision    |     8 байт     | вещественное число с переменной точностью                | точность в пределах 15 десятичных цифр                                 |
--------------------|----------------|----------------------------------------------------------|------------------------------------------------------------------------|
     smallserial    |    2 байта     | небольшое целое с автоувеличением                        | 1.. 32767                                                              |
--------------------|----------------|----------------------------------------------------------|------------------------------------------------------------------------|
         serial     |    4 байта     | целое с автоувеличением                                  | 1 .. 2147483647                                                        |
--------------------|----------------|----------------------------------------------------------|------------------------------------------------------------------------|
      bigserial     |     8 байт     | большое целое с автоувеличением                          | 1.. 9223372036854775807                                                |
--------------------|----------------|----------------------------------------------------------|------------------------------------------------------------------------|

Выбор между целочисленными типами достаточно прост: если все ожидаемые значения в пределах сотни, то не надо резервировать под них 8-байтовый bigint. Как правило, стандартного 4-байтового integer достаточно для большинства задач.
numeric стоит использовать для различных "денежных" вещей, где недопустимо "потерять копейку на округлениях":

SELECT 3.1415926::real;
--     3.1415925 - чуток потеряли
SELECT 3.1415926::numeric;
--     3.1415926 - а тут все четко


Еще пара вещей может вызвать недоумение у людей с опытом программирования:
- serial-псевдотипы (аналог AUTO_INCREMENT / IDENTITY из других СУБД), которые позволяют определить поля с автоматически формируемым возрастающим значением "по умолчанию": 1, 2, 3, ...
- нет unsigned - все числовые типы знаковые, поэтому "честно" положить диапазон [0x00000000..0xFFFFFFFF] в integer не получится, только со смещением "наполовину"


2) Символьные типы
Символьные/строковые типы представлены парой описанных в стандарте char/varchar и парой PostgreSQL-специфичных bpchar/text.

                    Имя              |                             Описание                                               |
-------------------------------------|------------------------------------------------------------------------------------|
character varying(n), varchar(n)     | строка ограниченной переменной длины                                               |
-------------------------------------|------------------------------------------------------------------------------------|
character(n), char(n), bpchar(n)     | строка фиксированной длины, дополненная пробелами                                  |
-------------------------------------|------------------------------------------------------------------------------------|
             bpchar                  | строка неограниченной переменной длины с удалением пробелов                        |
-------------------------------------|------------------------------------------------------------------------------------|
             text                    | строка неограниченной переменной длины                                             |
-------------------------------------|------------------------------------------------------------------------------------|

Если вы не предполагаете перенос вашего приложения на другую СУБД, то можете спокойно использовать тип text везде, поскольку указание ограничения длины не дает никаких бонусов. Конечно, за исключением случаев, когда вам действительно 
требуется ограничить длину записываемого в поле - например, для 2-буквенного кода страны.


3) Типы даты/времени
Дата и время в PostgreSQL, технически, хранятся как целочисленные, со значением от POSTGRES_EPOCH (01.01.2000) в соответствующих единицах (микросекундах или сутках):

                Имя                          | Размер    |                 Описание                         |    Наименьшее значение      |     Наибольшее значение     |      Точность       |
---------------------------------------------|-----------|--------------------------------------------------|-----------------------------|-----------------------------|---------------------|
 timestamp [ (p) ] [ without time zone ]     | 8 байт    |       дата и время (без часового пояся)          |            4713 до н.э.     |            294276 н.э.      | 1 микросекунда      |
---------------------------------------------|-----------|--------------------------------------------------|-----------------------------|-----------------------------|---------------------|
    timestamp [ (p) ] with time zone         | 8 байт    |        дата и время (с часовым поясом)           |            4713 до н.э.     |            294276 н.э.      | 1 микросекунда      | 
---------------------------------------------|-----------|--------------------------------------------------|-----------------------------|-----------------------------|---------------------|
                   date                      | 4 байта   |             дата (без времени)                   |            4713 до н.э.     |           5874897 н.э.      |       1 день        |
---------------------------------------------|-----------|--------------------------------------------------|-----------------------------|-----------------------------|---------------------|
        time [ (p) ] [ without time zone ]   | 8 байт    |             время суток (без даты)               |            00:00:00         |              24:00:00       | 1 микросекунда      |
---------------------------------------------|-----------|--------------------------------------------------|-----------------------------|-----------------------------|---------------------|
         time [ (p) ] [ with time zone ]     | 12 байт   |       время дня (без даты), с часовым поясом     |         00:00:00+1559       |          24:00:00-1559      | 1 микросекунда      |
---------------------------------------------|-----------|--------------------------------------------------|-----------------------------|-----------------------------|---------------------|
            interval [ поля ] [ (p) ]        | 16 байт   |            временной интервал                    |         -178000000 лет      |          178000000 лет      | 1 микросекунда      |
---------------------------------------------|-----------|--------------------------------------------------|-----------------------------|-----------------------------|---------------------|

В этом их отличие от некоторых других СУБД, где timestamp может храниться как текстовая строка. 
А раз это просто числа, то арифметические операции над ними тоже допустимы, в том числе преобразование к Unix time (время от 01.01.1970) :

SELECT '2024-01-01'::date - 1;
-- 2023-12-31 - за день до
SELECT '2024-01-01'::date - 8 * '1 hour'::interval;
-- 2023-12-31 16:00:00 - за 8 часов до
SELECT extract(epoch from '2024-01-01'::timestamp);
-- 1704067200 - превратили timestamp в double precision
SELECT '1970-01-01 00:00:00'::timestamp + 1704067200 * '1 second'::interval;
-- 2024-01-01 00:00:00 - ... и обратно


Опционально, во временном значении можно использовать часовой пояс (with time zone) или указывать сохраняемую точность (timestamp(0) означает хранение "до секунд").


4) Логический тип

Логические значения представлены типом boolean:

   Имя    | Размер   |              Описание             |
----------|----------|-----------------------------------|
boolean   | 1 байт   | Состояние: истина или ложь        |
----------|----------|-----------------------------------|

Он может принимать значения TRUE/FALSE и, с учетом SQL-специфики, значение NULL, равно как и любой другой тип.


5) Специальные типы данных

Помимо базовых типов, "из коробки" PostgreSQL предоставляет массу других, более специализированных, типов: двоичные данные, перечисления, геометрические, сетевые адреса, битовые строки, вектора текстового поиска, UUID, XML, JSON, 
массивы, диапазоны.

Например, всякие картографические сервисы любят использовать геометрические типы данных с расширением PostGIS, а слабоструктурированные данные можно хранить в JSON, причем ничуть не хуже какой-нибудь MongoDB, а идентификаторы в 
распределенных системах - в UUID.

Если вдруг и этих типов вам окажется мало – можно создать свой и работать с ним как с любым другим полем. Главное, правильно его описать, задать соответствующие функции ввода-вывода, хранения и обработки. Вообще, PostgreSQL очень хорошо 
расширяем, поэтому EXTENSION'ы, которые для него можно найти и подключить, составляют достаточно весомую часть его преимуществ по отношению к другим СУБД.




______________________________________________________________________________________
Базовые SQL-команды (#2)

Давайте снова вернемся к нашей демо-базе и наконец добавим туда хоть какие-то данные.


1) INSERT
За добавление данных, за их вставку в таблицу, в SQL отвечает команда INSERT:

INSERT INTO tbl(      -- куда будем вставлять данные
  k                   -- имена полей
, v
)
VALUES                -- перечисляем вставляемые строки
  (1, '1st string')
, (102, 'another string')
, (3, NULL);          -- вовсе не ''

Мы указываем, в какую таблицу и в какие поля должны быть добавлены данные, и, в простейшей форме INSERT … VALUES, прямо перечисляем те строки, которые хотим вставить. Значения в них позиционно соответствуют указанным полям таблицы.
Да вот беда – рука у нас дрогнула, и вместо "2" у нас вставилось "102", а третья строка у нас вставилась вообще без данных в текстовое поле… Замечу, что пустая строка и NULL – совсем разные значения.


2) UPDATE
Давайте мы эти данные поправим. А за изменение каких-то данных в SQL отвечает команда UPDATE:

UPDATE
  tbl
SET
  k = k - 100      -- правила изменения значений полей
, v = '2nd string'
WHERE
  k = 102;         -- условие отбора строк

В ней мы указываем, в какой таблице (а таблица - это ключевой момент работы с SQL, и практически все, что придется делать, мы будем делать именно с таблицей) и как мы хотим изменить поля. При этом полей можно менять сразу несколько. То 
есть если в некоторых императивных языках программирования деструкция объектов только-только занимает свое место в стандартах, то в SQL это было всегда - "у такого-то набора полей задай новые значения вот так-то". И, дополнительно, 
описываем условие, которое должно отобрать только те строки, для которых наша операция должна отработать. В данном случае мы для строки с k = 102 (обратите внимание на одинарное равенство при сравнении) хотим изменить строковое 
значение v на новое, а из значения k вычесть 100 (присвоение точно так же описывается одинарным символом равенства).


3) DELETE
А последняя строка, в которую у нас "просочился" NULL вместо текстовой строки, нам вообще не нужна. Давайте ее просто удалим - для этого есть команда DELETE:

DELETE FROM
  tbl
WHERE
  v IS NULL;

Обратите внимание, что с NULL-значениями нельзя пользоваться обычными операторами типа "равно"/"не равно", для них есть свои операторы IS. В данном случае мы используем IS NULL, чтобы проверить на совпадение с NULL-значением.


###################
NULL-логика

Потому что базовые операторы (=, <>, NOT) выдают значение NULL, если его имеет хотя бы один из аргументов. А при приведении типов в условии оно превращается в "ложь", и ни одну запись вы не отберете. Поэтому для сравнений NULL и с ним 
(в конкретном поле или для всей строки сразу) стоит использовать операторы IS/IS NOT или IS DISTINCT FROM/IS NOT DISTINCT FROM. Некоторые вещи не всегда очевидны, поэтому вот шпаргалка, которой можно пользоваться:


       |  T  |  F  |  N  |
-------|-----|-----|-----|
  NOT  |  F  |  T  |  N  |



   =   |  T  |  F  |  N  |
-------|-----|-----|-----|
   T   |  T  |  F  |  N  |
-------|-----|-----|-----|
   F   |  F  |  T  |  N  |
-------|-----|-----|-----|
   N   |  N  |  N  |  N  |
-------|-----|-----|-----|



  и IS NOT DISTINCT FROM

   IS   |  T  |  F  |  N  |
--------|-----|-----|-----|
   T    |  T  |  F  |  F  |
--------|-----|-----|-----|
   F    |  F  |  T  |  F  |
--------|-----|-----|-----|
   N    |  F  |  F  |  T  |
--------|-----|-----|-----|




  <>  |  T  |  F  |  N  |
------|-----|-----|-----|
   T  |  F  |  T  |  N  |
------|-----|-----|-----|
   F  |  T  |  F  |  N  |
------|-----|-----|-----|
   N  |  N  |  N  |  N  |
------|-----|-----|-----|




и IS DISTINCT FROM


  IS NOT  |  T  |  F  |  N  |
----------|-----|-----|-----|
       T  |  F  |  T  |  T  |
----------|-----|-----|-----|
       F  |  T  |  F  |  T  |
----------|-----|-----|-----|
       N  |  T  |  T  |  F  |
----------|-----|-----|-----|


coalesce(a, <const>) = <const> ~
	a IS NULL OR
	a = <const>


a IS DISTINCT FROM b ~
	(a IS          NULL AND b IS NOT NULL) OR
	(a IS NOT NULL AND b IS          NULL) PR
	(a IS NOT NULL AND b IS NOT NULL AND a <> b) 



	 	                              |  N и N | N и T/F | T/F и N | T/F и T/F |
----------------------------------------------|--------|---------|---------|-----------|
IS NULL                                       |    T   |    F    |    F    |     F     |
----------------------------------------------|--------|---------|---------|-----------|
IS NOT NULL                                   |    F   |    F    |    F    |     T     |
----------------------------------------------|--------|---------|---------|-----------|
IS DISTINCT FROM NULL                         |    T   |    T    |    T    |     T     |
----------------------------------------------|--------|---------|---------|-----------|
IS NOT DISTINCT FROM NULL                     |    F   |    F    |    F    |     F     |
----------------------------------------------|--------|---------|---------|-----------|
IS DISTINCT FROM (NULL, NULL)                 |    F   |    T    |    T    |     T     |
----------------------------------------------|--------|---------|---------|-----------|
IS NOT DISTINCT FROM (NULL, NULL)             |    T   |    F    |    F    |     F     |
----------------------------------------------|--------|---------|---------|-----------|

(a,b) IS DISTINCT FROM (c,d) ~
	(a IS DISTINCT FROM c) OR
	(b IS DISTINCT FROM d)



Конец NULL-логика
###################




4) ... RETURNING

А что, если мы хотели не просто удалить записи, но и узнать, какие именно были удалены?.. 
Чтобы СУБД ответила нам не просто "я вставила/обновила/удалила две строки", а "я обработала две вот такие строки", необходимо воспользоваться ключевым словом RETURNING и перечислить те поля, которые мы хотим увидеть:

DELETE FROM
  tbl
WHERE
  v IS NULL
RETURNING *;

В данном случае мы используем "*", которая в SQL означает "все поля". В данном случае мы увидим, что удаляется одна строка со значением 3 в поле k:

   k    |  v
integer | text
      3 |



5) SELECT
И, наконец-то, мы можем сделать то, ради чего мы базу-то и создавали - что-то из нее взять, выбрав из нашей таблицы какие-то записи. Для этого воспользуемся командой SELECT, про которую говорили ранее, опять же, указав "*" вместо 
списка полей:

SELECT
  *
FROM
  tbl;

И мы увидим то, что осталось в таблице после всех наших манипуляций:

k       | v
integer | text
      1 | 1st string
      2 | 2nd string

Мы видим две строки, которые соответствуют тому условию, которое мы задали для выборки. Только что-то никакого условия в запросе мы при этом не видим... и это мы обсудим на следующей лекции.




______________________________________________________________________________________
###################################################################
______________________________________________________________________________________
Простые SELECT

Сегодня поговорим о самых простых, но важных, возможностях команды SELECT, наиболее часто используемой при работе с базами данных - формировании выборок (VALUES), их ограничении (LIMIT/OFFSET/FETCH), фильтрации (WHERE/HAVING), 
сортировке (ORDER BY), уникализации (DISTINCT) и группировке (GROUP BY).

На прошлой лекции мы остановились на том, что прочитали из таблицы все данные. И выяснили, что на SQL сделать это очень просто - не надо писать ни циклов, ни итераторов, достаточно всего лишь:

SELECT * FROM <имя_таблицы>;

Впрочем, можно даже не писать SELECT * FROM, потому что есть команда TABLE, которая делает то же самое - безусловно вычитывает из таблицы строки со всеми полями в них:

TABLE <имя_таблицы>;

Но если вдруг кому-то начало казаться, что SELECT - это просто, то это совсем не так, это достаточно сложно, ведь SELECT - самая богатая по количеству функционала команда, которая только есть в SQL:

[ WITH [ RECURSIVE ] запрос_WITH [, ...] ]
SELECT [ ALL | DISTINCT [ ON ( выражение [, ...] ) ] ]
	[ * | выражение [ [ AS ] имя_результата ] [ , ... ] ]
	[ FROM элемент_FROM [, ...] ]
	[ WHERE условие ]
	[ GROUP BY [ ALL | DISTINCT ] элемент_группирования [, ...] ]
	[ HAVING условие ]
	[ WINDOW имя_окна AS ( определение_окна )  [, ...] ]
	[ { UNION | INTERSECT | EXCEPT }  [ ALL | DISTINCT ] выборка ] 
	[ ORDER BY выражение [ ASC | DESC | USING оператор ] [ NULLS { FIRST | LAST } ] [, ...] ]
	[ LIMIT { число | ALL } ]
	[ OFFSET начало [ ROW | ROWS ] ]
	[ FETCH { FIRST | NEXT } [ число ] { ROW | ROWS } { ONLY | WITH TIES } ]
	[ FOR { UPDATE | NO KEY UPDATE | SHARE | KEY SHARE } [ OF имя_таблицы [, ...] ] [ NOWAIT | SKIP LOCKED ]  [...] ]


Но если вы почему-то думаете, что это все - какие-то модные нововведения в связи с развитием поддержки расширений стандарта SQL, то это тоже не так. Всех нововведений на этом слайде, относительно наиболее старой поддерживаемой сейчас 
версии PostgreSQL 11 всего два: опция WITH TIES, добавленная в версии PostgreSQL 13 возможность неразрывного захвата группы "одинаковых" записей (что удобно для "постраничной навигации", например), и расширение GROUP BY DISTINCT, 
представленное в PostgreSQL 14.


1) VALUES
Но давайте наше рассмотрение функционала команды SELECT вообще не с нее, с совсем другой команды - VALUES. Если погрузиться глубоко в исходники PostgreSQL, то VALUES, как и TABLE, это такой маленький SELECT, только с куцым функционалом:

VALUES ( выражение [, ...] ) [, ...]
	[ ORDER BY выражение_сортировки [ ASC | DESC | USING оператор ] [, ...] ]
	[ LIMIT { число | ALL } ]
	[ OFFSET начало [ ROW | ROWS ] ]
	[ FETCH { FIRST | NEXT } [ число ] { ROW | ROWS } ONLY ]


Из всего многообразия возможностей SELECT, для VALUES оставлено всего лишь 4, как это в русской документации называется, "предложения" (в оригинале - "clause"). Что они позволяют делать, мы и рассмотрим, для начала. Вообще, 
команда VALUES позволяет вручную определить некоторую выборку, ее формат, и из чего она будет состоять: через запятую внутри скобок - значения полей записи, вне скобок - перечисление самих записей:

VALUES (1);	
-------------
column1
integer
      1


VALUES (1,2);
----------------
column1 | column2
integer | integer
      1 |       2



VALUES (1), (2);
------------------
column1
integer
      1
      2


VALUES
    (1, 2.5, 'alpha'),
    (4, 5.5, 'beta');
--------------------
column1 | column2 | column3
integer | numeric | text
      1 |     2.5 | alpha
      4 |     5.5 | beta




VALUES
    (1, 2.5, 'alpha'),
    (4, 5.5);
---------------------
ERROR: VALUES lists must all be the same length LINE 3: , (4, 5.5)


Заодно, она самостоятельно задает имена столбцов (column1, column2, ... и далее по порядку) и их типы - если мы не указали специально тип значения в поле, то будет выбран наиболее подходящий из трех стандартных вариантов: integer, 
numeric или text. Как можно видеть на слайде, любые лишние пробелы или переводы строк (кроме участвующих в текстовых литералах, конечно) SQL не волнуют вообще никак, поэтому код запросов можно писать с таким форматированием, которое 
удобно вам или вашей команде. Однако, при описании выборок таким способом, надо быть особо внимательным. Потому что если вдруг вы ошибетесь в типах значений, то иногда они приведутся (и не факт, что вы этого хотели), а иногда вы 
получите-таки ошибку:

VALUES (1), ('1');
-- 1
-- 1
VALUES (1), ('a');
-- ERROR: invalid input syntax for type integer: "a"

Еще одна проблема может поджидать любителей генерировать текст запроса в коде, если ваша стандартная функция свертки массива значений полей нехорошо обходится с NULL или undefined - вы рискуете получить или пустоту вместо значения, или 
меньше необходимого количества столбцов, что одинаково приведет к ошибке выполнения на стороне сервера СУБД:

VALUES (1,2),(3,);
-- ERROR: syntax error at or near ")"
VALUES (1,2),(3);
-- ERROR: VALUES lists must all be the same length

Помимо статичных значений, в VALUES могут также использоваться NULL-значения, числовые значения в экспоненциальной форме и результаты вычисления выражений:

VALUES
    (1, 2.5e+0, 'al' || 'pha'),
    (2 + 2, NULL, 'beta');
----------------------------------
column1 | column2 | column3
integer | numeric | text
      1 |     2.5 | alpha
      4 |         | beta


Здесь мы видим два выражения: 2 + 2 и 'al' || 'pha'. Причем второе, с оператором ||, который во многих привычных языках программирования означает "логическое ИЛИ", в PostgreSQL - конкатенация строк.



______________________________________________________________________________________
Операторы в PostgreSQL

Если вас это почему-то смущает, то стоит погрузиться в документацию, где операторам, как они могут выглядеть и из чего состоять, посвящен достаточно большой абзац:

Имя оператора образует последовательность не более чем NAMEDATALEN-1 (по умолчанию 63) символов из следующего списка:

+ - * / < > = ~ ! @ # % ^ & | ` ?

Однако для имён операторов есть ещё несколько ограничений:
- Сочетания символов -- и /* не могут присутствовать в имени оператора, так как они будут обозначать начало комментария.
- Многосимвольное имя оператора не может заканчиваться знаком + или -, если только оно не содержит также один из этих символов: ~ ! @ # % ^ & | ` ?
   Например, @- — допустимое имя оператора, а *-  — нет. Благодаря этому ограничению, PostgreSQL может разбирать корректные SQL-запросы без пробелов между компонентами.

Даже "из коробки" операторов в PostgreSQL достаточно много, но если вам их почему-то не хватило, вы можете создать свой, лишь бы он удовлетворял описанным выше правилам. При этом даже среди "стандартных" операторов есть достаточно 
"заковыристые" варианты. Например, из этих четырех вы будете активно использовать, а с тремя другими, надеюсь, не столкнетесь никогда:

VALUES (|/ 36, ||/ 125, @ -1, 'al' || 'pha');
----------------------------------------------------------
column1           |  column2          | column3 | column4
double precision  | double precision  | integer | text
               6  |                5  |       1 | alpha


Потому что операторы квадратного и кубического корней и абсолютного значения короче, да и понятнее заменить на соответствующие им функции, чего как раз не скажешь о конкатенации, поскольку писать тогда приходится больше:

VALUES (sqrt(36), cbrt(125), abs(-1), concat('al', 'pha'));

Раз мы заговорили об операторах, стоит упомянуть, что они имеют различный приоритет (от большего к меньшему):

           Оператор/элемент           |    Очерёдность    |                            Описание                                    |
--------------------------------------|-------------------|------------------------------------------------------------------------|
.                                     | слева-направо     | разделитель имён таблицы и столбца                                     |
--------------------------------------|-------------------|------------------------------------------------------------------------|                                             
::                                    | слева-направо     | приведение типов в стиле PostgreSQL                                    |
--------------------------------------|-------------------|------------------------------------------------------------------------|
[ ]                                   | слева-направо     | выбор элемента массива                                                 |
--------------------------------------|-------------------|------------------------------------------------------------------------|
+ -                                   | Справа-налево     | унарный плюс, унарный минус                                            |
--------------------------------------|-------------------|------------------------------------------------------------------------|
^                                     | слева-направо     | возведение в степень                                                   |
--------------------------------------|-------------------|------------------------------------------------------------------------|
* / %                                 | слева-направо     | умножение, деление, остаток от деления                                 |
--------------------------------------|-------------------|------------------------------------------------------------------------|
+ -                                   | слева-направо     | сложение, вычитание                                                    |
--------------------------------------|-------------------|------------------------------------------------------------------------|
(любой другой оператор)               | слева-направо     | все другие встроенные и пользовательские операторы                     |
--------------------------------------|-------------------|------------------------------------------------------------------------|
BETWEEN IN LIKE ILIKE SIMILAR         |                   | проверка диапазона, проверка членства, сравнение строк                 |
--------------------------------------|-------------------|------------------------------------------------------------------------|
<   >   =    <=    >=    <>           |                   | операторы сравнения                                                    |
--------------------------------------|-------------------|------------------------------------------------------------------------|
IS ISNULL NOTNULL                     |                   | IS TRUE, IS FALSE, IS NULL, IS DISTINCT FROM и т.д.                    |
--------------------------------------|-------------------|------------------------------------------------------------------------| 
NOT                                   | Справа-налево     | логическое отрицание                                                   |
--------------------------------------|-------------------|------------------------------------------------------------------------|
AND                                   | слева-направо     | логическая конъюнкция                                                  |
--------------------------------------|-------------------|------------------------------------------------------------------------|
OR                                    | слева-направо     | логическая дизъюнкция                                                  |
--------------------------------------|-------------------|------------------------------------------------------------------------|

Причем все из них, кроме унарных +/- и NOT, вычисляются слева-направо. Оператор OR имеет минимальный приоритет, поэтому всегда будет выполняться в последнюю очередь.



______________________________________________________________________________________
Порядок вывода строк (ORDER BY)

Но давайте все-таки вернемся к возможностям команды VALUES и рассмотрим первое "предложение" - ORDER BY, которое позволяет задать порядок вывода получаемых нами записей.

ORDER BY
    выражение                                           -- номер столбца выборки, имя поля или выражения
        [ ASC | DESC | USING оператор ]    		-- порядок сортировки [ASC]
        [ NULLS { FIRST | LAST } ]                      -- куда относить NULL-значения [ASC, LAST, DESC FIRST]
[, ...]

Оно состоит из списка выражений сортировки, которые применяются последовательно - то есть сначала все записи сортируются по первому выражению, при равенстве значений в первом - по второму, и так далее. Каждое из таких выражений может 
быть представлено:
- позиционным номером или именем столбца сортируемой выборки
- некоторым выражением от одного или нескольких полей строки (столбцов выборки) с операторами и функциями

Единственное требование - лишь бы тип столбца или результата выражения поддерживал операторы линейного порядка <, <=, =, >=, >. То есть, например, boolean, числа или строки отсортировать "по порядку" можно, а двухкоординатные 
геометрические точки - нельзя.

Дополнительно у нас могут быть указаны ключевые слова:
- ASC/DESC, которые позволяют определить направление сортировки - по возрастанию (по умолчанию) или убыванию соответственно
- NULLS FIRST/NULLS LAST, определяющие положение NULL-значений раньше/позже всех остальных

Если же и после сортировки по последнему из выражений в нем оказались равные значения или если не было ORDER BY в запросе вовсе, то порядок записей не определен. То есть, в соответствии со стандартом SQL, база вправе отдавать вам 
записи в любом произвольном, удобном ей с точки зрения производительности, порядке, не нарушающем указанный в запросе. Не указали ORDER BY и надеетесь, что записи сохранят хоть какой-то порядок при следующем выполнении того же 
запроса?.. В большинстве случаев - зря! Для базы нет никакой разницы между "эта запись у меня записана в начале таблицы, а эта - в конце, значит, она будет выведена позже!" Да и вообще, ничто не заставляет PostgreSQL читать файл 
таблицы "с начала" - чтение запросто может стартовать и с его "середины".

Выполним нашу команду VALUES без всякого указания ORDER BY - и, несмотря на все сказанное ранее, получаем строки ровно в том порядке, в котором их указали в запросе! Такое поведение не регламентируется стандартом, но реализовано в 
PostgreSQL "by design". Однако, при наличии любой сортировки порядок данных в неперечисленных там столбцах будет произвольным. Например, при сортировке по первому столбцу "по возрастанию" можно внезапно обнаружить, что второй столбец 
оказался отсортированным "по убыванию".

VALUES                                                                                              VALUE
    (1, 2, 1),                                                                                      	(1, 2, 1),
    (2, 1, 2),                                                                                          (2, 1, 2),
    (1, 1, 3);                                                                                          (1, 1, 3)
                                                                                                    ORDER BY
                                                                                                         1;             -- сортировка по первому столбцу
----------------------------                                                                        ------------------------------------------------------
column1 | column2 | column3                                                                         column1 | column2 | column3
integer | integer | integer                                                                         integer | integer | integer
      1 |       2 |       1                                                                               1 |       2 |        1
      2 |       1 |       2                                                                               1 |       1 |        3
      1 |       1 |       3                                                                               2 |       1 |        2



Также необходимо помнить, что если у нас возникает задача "развернуть" выборку в обратном порядке, то это надо проделать со всем перечнем сортирующих выражений:

-- прямой порядок
ORDER BY 1 ASC, 2 ASC
-- обратный порядок
ORDER BY 1 DESC, 2 DESC

При этом, если мы используем однонаправленную сортировку сразу по нескольким столбцам, их можно "объединить в кортеж" и "развернуть" сразу вместе:

-- прямой порядок
ORDER BY (column1, column2) -- ASC по умолчанию
-- обратный порядок
ORDER BY (column1, column2) DESC

Кстати насчет порядка... Учитывая два направления сортировки и два варианта позиции NULL-значений, мы получаем 4 варианта сортировок для любого выражения:

ORDER BY x                 -- ASC  NULLS LAST
ORDER BY x NULLS FIRST     -- ASC  NULLS FIRST
ORDER BY x DESC NULLS LAST -- DESC NULLS LAST
ORDER BY x DESC            -- DESC NULLS FIRST

... и все они разные:

              |  NULLS |  NULLS |
              |  LAST  |  FIRST |
     ---------|--------|--------|
              |   1    |  NULL  |
              |   2    |  NULL  |
              |   3    |   1    |
       ASC    |   4    |   2    |
              |  NULL  |   3    |
              |  NULL  |   4    |
      --------|--------|--------|
              |   4    |  NULL  |
              |   3    |  NULL  |
              |   2    |   4    |
        DESC  |   1    |   3    |
              |  NULL  |   2    |
              |  NULL  |   1    |
      --------|--------|--------| 


Заметьте, что обратной к "умолчательной" ASC NULLS LAST является DESC NULLS FIRST, что бывает удивительным для начинающих разработчиков, обнаруживающих NULL'ы в самом начале "развернутой" выборки.




###################
Сортировка строковых значений

Если мы используем "обычную" сортировку, то порядок строк будет алфавитным, причем строчные и прописные буквы окажутся рядом. Но если воспользуемся "побайтовой" сортировкой USING ~<~, то получим весьма неожиданный результат:

VALUES                                                                   VALUES 
    ('ёж'),                                                              	('ёж'),
    ('ель'),                                                                    ('ель'),
    ('ял'),                                                                     ('ял'),
    ('alpha'),                                                                  ('alpha'),
    ('Яков')                                                                    ('Яков')
ORDER BY                                                                 ORDER BY
    column1; -- [USING <]                                                	column1 USING ~<~;
---------------------------------		                        ---------------------------------
alpha                                                                   alpha
ель                                                                     Яков
ёж                                                                      ель 
Яков                                                                    ял
ял                                                                      ёж  


Думаю, многие догадываются, что зачатки такого положения вещей были посеяны еще при портировании IBM XT и MS DOS на их советские аналоги типа ЕС-1840. Однако, русским языком проблемы не ограничиваются и точно так же страдают поляки, 
немцы и многие другие народы, пишущие "с умляутами", не говоря уж про пользователей RTL-языков. Поэтому был разработан такой инструмент, как правила сортировки, которые, фактически, определяют позицию каждого символа в кодовой таблице. 
Вы можете установить соответствующую кодовую таблицу на сервер PostgreSQL и использовать для сортировки. Например, установленная по умолчанию POSIX/C-кодировка может быть использована как ORDER BY column1 COLLATE "C" и даст результат 
аналогичный "побайтовой" ORDER BY column1 USING ~<~.


______________________________________________________________________________________
Ограничения результата (LIMIT/OFFSET)

Второй блок "предложений" (clause) для VALUES позволяет ограничить перечень строк в результирующей выборке: сколько штук хотим получить и с какой по счету начинать.

[ LIMIT {число | ALL} ]
[ OFFSET начало [ ROW | ROWS ] ]
[ FETCH { FIRST | NEXT } [ число ] { ROW | ROWS } { ONLY | WITH TIES } ]


Ограничение количества строк (LIMIT)
Чтобы задать желаемое количество строк ответа (например, вы хотите выбрать из таблицы какие-то 5 строк, хотя их там миллионы), воспользуемся ключевым словом LIMIT:

-- чтобы вернуть не более 5 строк
LIMIT 5
LIMIT '5'
-- чтобы вернуть все строки без ограничения
LIMIT NULL
LIMIT ALL
    -- ... или просто не указывать LIMIT


"Не более" означает, что даже если мы установим лимит в 5 строк для выборки, где их всего 3, то и получим только 3 - ведь базе неоткуда взять еще 2.



Смещение начала (OFFSET)
Чтобы начать получать строки не с первой по порядку, воспользуемся ключевым словом OFFSET:

-- начинаем с 6-й строки (пропускаем 5)
OFFSET 5
OFFSET '5'
-- начинаем с самого начала
OFFSET NULL
OFFSET 0           
-- ... или просто не указывать OFFSET


Некоторые почему-то считают, что OFFSET нельзя указывать без LIMIT, но это не так:

VALUES
    ('a'),
    ('b'),
    ('c')
OFFSET 1;
-- b
-- c


Понятно, что при LIMIT 0 или OFFSET ALL результат окажется заведомо пустым. Впрочем, первый вариант можно использовать, если вам необходимо получить только формат столбцов результата запроса, а вот второй вызовет ошибку:

ERROR:  syntax error at or near "ALL"
LINE 1: OFFSET ALL;

В целом, можно вычислить, что при исходном размере выборки S строк, заданном лимите L и смещении O, мы всегда получаем min(max(S - O, 0), L) строк результата:

VALUES
    ('a'),
    ('b'),
    ('c')
OFFSET 1;
------------------------
b
c

  -------------| |-----|------|-------| |-------------
   LIMIT ALL   | |     |  1   |       | |
               | |     |------|       | |     OFFSET 2
               | |     |  2   |       \/
               | |     |------|-----------------------           
               | |     |  3   |        | |
               | |     |------|        | |
               | |     |  4   |        | |
               | |     |------|        | |
               \/      |  5   |        \/
  ---------------------|------|--------------------------






VALUES
    ('a'),
    ('b'),
    ('c')
LIMIT 1
OFFSET 1;
--------------
b

  -------------| |-----|------|-------| |-------------
  LIMIT 2      | |     |  1   |       | |
               | |     |------|       | |     OFFSET 2
               \/      |  2   |       \/
  ---------------------|------|-----------------------           
                       |  3   |       | |
                       |------|       | |      LIMIT 2
                       |  4   |       \/
  ---------------------|------|-----------------------
                       |  5   |
                       |------| 


И такой подход позволяет достаточно просто реализовывать постраничную навигацию, если порядок выборки у вас стабилен (например, вы задали правильный однозначный и стабильный порядок с помощью ORDER BY):

...
ORDER BY ...
LIMIT  <rows_per_page>
OFFSET <rows_per_page * page_num>



______________________________________________________________________________________
FETCH

Если изначально стандарт SQL предполагал использование только пары LIMIT и OFFSET, то с SQL:2008 появилась комбинация OFFSET и ключевого слова FETCH, которое по смыслу аналогично LIMIT с небольшими дополнениями:

...
OFFSET 1 ROWS
FETCH FIRST 1 ROWS ONLY
-- полностью тоже самое, что 
...
LIMIT 1
OFFSET 1

Тут слова ROWS и FIRST незначимы, и поддерживаются исключительно ради соответствия стандарту, а вот ONLY - ой как значимо! Именно эта форма дает результат эквивалентный LIMIT + OFFSET.
А вот вторая форма WITH TIES, появившаяся в PostgreSQL 13, заставляет базу отдавать сразу все строки неразличимого для сортировки блока, который попадает на границу выборки. Эта возможность позволяет существенно упростить реализацию 
постраничной навигации:

VALUES                                                               VALUES
    ('a', 1),                                                        	('a', 1),
    ('b', 2),                                                           ('b', 2),
    ('a', 3),                                                           ('a', 3),
    ('b', 4)                                                            ('b', 4)
ORDER BY 1                                                           ORDER BY 1
FETCH FIRST 1 ROWS                                            		FETCH FIRST 1 ROWS
    ONLY;                                                                  WITH TIES;      -- !!!
-------------------------                                          -----------------------------
column1 | column2                                              	     column1 | column2
text    | integer                                                       text | integer
      a |       1                                                          a |       1
                                                                           a |       3




______________________________________________________________________________________
SELECT (#2)

Давайте, наконец, вернемся от примитивного VALUES к полноценному SELECT, который уже посложнее.


Формат результирующей выборки
Как минимум, он отличается тем, что мы описываем те столбцы, которые хотим в результирующей выборке видеть - список возвращаемых полей. Если VALUES формирует имена и определяет типы столбцов самостоятельно, то для SELECT нам необходимо 
хоть что-то написать самим относительно того, что пришло во FROM-части.

Это может быть некоторое:
- выражение от столбцов из FROM,
- имя любого из этих столбцов
- или можем указать "*" ("дай мне все столбцы из...") для части или всего FROM

В целом, если "на вход" из FROM нам пришла строка, то и "на выходе" мы получим строку соответствующую ей, как-то преобразованную указанными нами выражениями.

SELECT
    [ * | выражение [ [ AS ] имя_результата ] [, ...] ]
[ FROM элемент_FROM [, ...] ]



                                                                     .*                                 SELECT               FROM
  SELECT   |-----------|----------|----------------|      |-----------|---------|                       красный   <=====    красный
	   |  зелёный  |  жёлтый  |   фиолетовый   | 	  |  голубой  |  синий  |                       жёлтый    <=====    жёлтый
	   |-----------|----------|----------------|      |-----------|---------|                       зелёный   <=====    зелёный
                  ^          ^        ^         ^                     ^                                 голубой   <=====    голубой
                  |          |        |         |                     |                                 синий     <=====    синий
                  \----------|--------|----\    \---------------------|-----\
                             |        |    |                          |     |
	          /----------|--------/    |                          |     |
                             |             |                          |     |         
	   |-----------|----------|----------|            |-----------|---------|
    FROM   | красный   |  жёлтый  | зелёный  |            |  голубой  |  синий  |
	   |-----------|----------|----------|            |-----------|---------|



Хотя, FROM-части в SELECT-запросе может и не быть:

SELECT
    'a'
, 1;
--------------------------
?column? | ?column?
text     | integer
       a |        1




SELECT
    'a' AS s -- AS писать необязательно
, 1 i;
----------------------------------------------
s     | i
text  | integer
    a |       1




SELECT
    random();
------------------
random
double precision
0.35806417754693065



SELECT
    generate_series(1, 3) i;
-----------------------------
i
integer
      1
      2
      3


При этом мы можем задать имя столбца результирующей выборки с помощью ключевого слова AS, хотя и без него - тоже. А можем и не задавать - в этом случае, в отличие от VALUES, имя ему присвоено не будет, обращаться по нему мы не сможем, 
но получить значение - легко.

Как правило, все библиотеки и утилиты для работы с PostgreSQL, отдают результирующую выборку двумя массивами:
- один описывает порядок, имена и типы столбцов,
- а второй содержит строки ответа в виде объектов или массивов значений полей в соответствии с порядком описания столбцов.



Либо в качестве имени столбца будет взято имя вызываемой для его формирования функции (random, generate_series, coalesce, nullif, ...) или оператора CASE. Функция при этом может генерировать как единственное значение, так и сразу набор 
строк.




########################
Исходная выборка (FROM)

Раз SELECT умеет принимать выборку "на вход" во FROM, а VALUES - выдавать, давайте попробуем совместить эти две команды:

SELECT
    *
FROM (
    VALUES
        ('a', 1)
    ,   ('b', 2)
    ,   ('a', 3)
    ,   ('b', 4)
);
-- ERROR:  VALUES in FROM must have an alias
-- HINT:  For example, FROM (VALUES ...) [AS] foo.


Однако, это нельзя сделать просто так. Впрочем, сам PostgreSQL подсказывает нам, как исправить ошибку в запросе - присвоить алиасы для каждого элемента внутри FROM:

SELECT
    *                         -- все столбцы FROM
,   T.*                       -- столбцы FROM-элемента
,   T.column1 str    	      -- конкретный столбец
FROM (
    VALUES
        ('a', 1)
    ,   ('b', 2)
    ,   ('a', 3)
    ,   ('b', 4)
) T;                          -- тут тоже можно не писать AS

---------------------------------------------------------------------
column1 | column2 | column1 | column2 | str
text    | integer | text    | integer | text
      a |       1 |       a |       1 |    a
      b |       2 |       b |       2 |    b
      a |       3 |       a |       3 |    a
      b |       4 |       b |       4 |    b


При этом мы можем получить ситуацию дублирования не только данных, но и имен столбцов в ответе. Пока мы не попытаемся обратиться по какому-то столбцу по неоднозначному имени, это не вызывает проблем. Причем какие-то из столбцов можно 
просто переименовать, как в этом примере из column1 в str. Впрочем, на переименованиях можно сэкономить, задав имена столбцов прямо при определении алиаса:


SELECT
    T.*
FROM (
    VALUES
        ('a', 1)
    ,   ('b', 2)
    ,   ('a', 3)
    ,   ('b', 4)
) T(str, i);

------------------------------
str    |i
text   |integer
     a |      1
     b |      2
     a |      3
     b |      4





########################
Фильтрация исходной выборки (WHERE)

Следующее "предложение" - WHERE, определяет, какие из строк исходной выборки будут обработаны и сформируют строки результата, а какие будут отброшены. Если формат выборки определяет, какие столбцы мы получим, то WHERE - какие строки.
И правила тут просты - если указанное boolean-выражение дает для строки TRUE - она попадает в выборку, если FALSE или NULL - нет.

WHERE условие


FROM        WHERE
-------------------------------------------
красный     TRUE   -----------> красный
жёлтый      FALSE      /------> зелёный
зелёный     TRUE -----/   /---> голубой
голубой     TRUE---------/
синий       NULL



Попробуем отобрать только строки со значением 'a' в поле str

SELECT
    *
FROM (
    VALUES
        ('a', 1)
    ,   ('b', 2)
    ,   ('a', 3)
    ,   ('b', 4)
) T(str, i)
WHERE
    str = 'a';
--------------------
column1 | column2
text    | integer
      a |       1
      a |       2


Еще раз напомню, что для сравнения в SQL используется "одинарный" оператор "=". 





########################
Вычисление условий в SQL

Но как только мы добавляем еще одно условие, начинаются сложности:

WHERE условие1 AND условие 2

Если кто-то привык по императивным языкам программирования, что всегда сначала будет вычислено условие1 и только после него, в зависимости от результата, - условие2, то в SQL их ждет много сюрпризов.

В SQL порядок вычисления условий отдан на откуп реализации конкретной СУБД. Этому в документации целый абзац посвящен, а я про это даже писал отдельную статью (https://habr.com/ru/post/494776/):

Заметьте, что это отличается от «оптимизации» вычисления логических операторов слева направо, реализованной в некоторых языках программирования. Как следствие, в сложных выражениях не стоит использовать функции с побочными эффектами. 
Особенно опасно рассчитывать на порядок вычисления или побочные эффекты в предложениях WHERE и HAVING, так как эти предложения тщательно оптимизируются при построении плана выполнения. Логические выражения (сочетания AND/OR/NOT) в 
этих предложениях могут быть видоизменены любым способом, допустимым законами Булевой алгебры.

Когда порядок вычисления важен, его можно зафиксировать с помощью конструкции CASE (см. Раздел 9.18). Например, такой способ избежать деления на ноль в предложении WHERE ненадёжен:

SELECT ... WHERE x > 0 AND y/x > 1.5;

Безопасный вариант:

SELCET ... WHERE CASE WHEN x > 0 THEN y/x > 1.5 ELSE false END;

Применяемая так конструкция CASE защищает выражение от оптимизации, поэтому использовать её нужно только при необходимости. (В данном случае было бы лучше решить проблему, переписав условие как y > 1.5*x.)




########################
Исключение дублей (DISTINCT)

Еще одним способом сократить количество строк в результирующей выборке является их уникализация с помощью DISTINCT:

SELECT DISTINCT список_выборки ...
SELECT DISTINCT ON ( выражение [, выражение ...]) список_выборки ...


	           SELECT
	
	          DISTINCT                      FROM
                |------------|             |------------|
                |  красный   |  <======    |  красный   |
                |------------|             |------------|
                                           |  красный   |
                |------------|             |------------|
                |  зелёный   |  <======    |  зелёный   |
                |------------|             |------------|
                                           |  зелёный   |
                |------------|             |------------|
                |   синий    |  <======    |   синий    |
                |------------|             |------------| 




	          SELECT
	       DISTINCT ON                                  FROM
	|------------|-----------|              |------------|------------|   
	|  красный   |  жёлтый   |   <===\\     |  красный   |  красный   |
	|------------|-----------|       ||     |------------|------------|
			                 \\===  |  красный   |  жёлтый    | 
        |------------|------------|             |------------|------------|
        |  зелёный   |  зелёный   |  <=======   |  зелёный   |  зелёный   |
	|------------|------------|             |------------|------------|
                                                |  зелёный   |  голубой   |
        |------------|------------|             |------------|------------|
        |   синий    |   синий    |   <=======  |   синий    |   синий    |
        |------------|------------|             |------------|------------| 



Мы можем либо задать набор выражений (или просто столбцов), который определяет, что будет считаться дублем, а что нет. При этом, сопоставляя по набору, мы можем задать правило сортировки строк внутри "совпадающей" группы 
(с обязательно в начале идущими выражениями уникализации), первая из которых и будет возвращена:

SELECT DISTINCT ON(str)
    *
FROM (
    VALUES
        ('a', 1), ('b', 2)
    ,   ('a', 3), ('b', 4) -- неполные клоны 
) T(str, i);
----------------------------------------------
str   | i
text  | integer
   a  |       1
   b  |       2




SELECT DISTINCT ON(str)
    *
FROM (
    VALUES
        ('a', 1), ('b', 2)
    ,   ('a', 3), ('b', 4)
) T(str, i)
ORDER BY
    str, i DESC; -- оперделяем сортировку
-------------------------------------------------
str   | i
text  | integer
    a |       3
    b |       4


Напоминаю, что при отсутствии явно заданной сортировки, база вернет вам ту запись, которая удобна ей, а не вам.

Если же мы ON-выражения не указали, совпадение определяется по полному набору столбцов сразу. Из каждого набора "полных клонов" строк результирующей выборки будет оставлена ровно одна, отличающаяся от других хоть чем-то:

SELECT DISTINCT
    *
FROM (
    VALUES
        ('a', 1), ('b', 2)
    ,   ('a', 1), ('b', 2) -- полные клоны 
) T(str, i);
----------------------------------------------
str   | i
text  | integer
   a  |       1
   b  |       2


Правда, используя DISTINCT для исправления неаккуратной реализации соединений, о которых мы поговорим на следующей лекции, вы можете сильно просадить производительность запроса. Поэтому перед использованием DISTINCT лучше сначала всегда 
задуматься, нельзя ли дубли побороть как-то иначе, точно ли не ваша неаккуратность привела к их появлению. Тут еще необходимо отметить, что все NULL-значения считаются разными между собой - ведь при проверке равенства, как было 
показано на прошлой лекции, сравнение с NULL (даже другого NULL) всегда дает тоже NULL, что эквивалентно ложности выражения. То есть любые две строки, содержащие NULL, заведомо "не совпадают".




########################
Группировка строк (GROUP BY)

Помимо DISTINCT, уникализировать строки нам может помочь "предложение" GROUP BY.

GROUP BY элемент_группирования [, ...]

По описанию он очень похож на ORDER BY, там также можно указывать номера столбцов, их имена или выражения от них, но нужен он, чтобы всю нашу выборку "разложить на кучки" по совпадению значений выражений (см. абзац про сравнение NULL 
выше). А потом по каждой "кучке" посчитать некоторую агрегатную функцию (count, min, max, sum, avg). Например, это может быть количество строк в группе (count), минимальное/максимальное или среднее значение какого-то показателя 
(min/max/avg) или его полная сумма по группе (sum) - причем их можно считать одновременно. Или, если стандартных функций вам не хватает, можно определить свою.

SELECT
    str
,   min(i)
,   max(i)
,   sum(i)
FROM
    VALUES
        ('a', 1), ('b', 2)
    ,   ('a', 3), ('b', 4)
) T(str, i)
GROUP BY
    1; -- str
----------------------------------------
str   | min      | max      | sum
text  | integer  | integer  | integer
   a  |       1  |       3  |       4
   b  |       2  |       4  |       6



	/-       |------------|------------|               |------------|-------------------|
	|        |  красный   |  красный   |   ========>   |  красный   |   красно-жёлтый   |
       <         |------------|------------|               |------------|-------------------|
	|        |  красный   |  жёлтый    | 
  /-    \-       |------------|------------|               |------------|-------------------|
  |              |  зелёный   |  зелёный   |  ========>    |  зелёный   |   зелёно-голубой  |       Здесь почему то во второй ячейке не зелёно-голубой а тускло-зелёный (Это что смесь жёлто-зелёно-голубой?), я не совсем понял
 <	         |------------|------------|               |------------|-------------------|
  |              |  зелёный   |  голубой   |
  \-     /-      |------------|------------|               |------------|-------------------|  
        <        |   синий    |   синий    | ========>     |   синий    |      синий        |       Здесь почему то во второй ячейке написан сине-голубой, я не совсем понял
         \-      |------------|------------|               |------------|-------------------|       


Если же нам надо вычислить результат без разбиения на группы ("Моя фамилия ИТОГО!"), вы можете указать группировку по пустому набору GROUP BY () или не указывать GROUP BY вовсе. При этом результат получающейся после группировки выборки 
можно сразу отсортировать:

SELECT im sum(...) ... GROUP BY 1 ORDER BY 2 DESC
-- или, если само значение агрегата неважно
SELECT i ... GROUP BY 1 ORDER BY sum(...) DESC




########################
Фильтрация сгруппированных строк (HAVING)

Раз уж мы умеем сортировать сгруппированные строки, то почему бы их и не пофильтровать? И тут на помощь нам приходит "предложение" HAVING:

HAVING условие


	                     FROM                                      WHERE                                  GROUP BY                                       HAVING
	          |------------|------------|              |------------|------------|             |-----------|--------------------|
                  |  красный   |  красный   |   =======>   |  красный   |  красный   |   =======>  | красный   | красно-жёлтый      |
                  |------------|------------|              |------------|------------|             |-----------|--------------------|
                  |  красный   |  жёлтый    | 
                  |------------|------------|              |------------|------------|  
                  |  зелёный   |  зелёный   |   =======>   |  зелёный   | зелёный    |             |-----------|---------------------|             |-----------|---------------------|
	          |------------|------------|              |------------|------------|   =======>  |  зелёный  | зелёно-голубой      |  =======>   |  зелёный  |    зелёно-голубой   |     
                  |  зелёный   |  голубой   |   =======>   |  зелёный   | голубой    |             |-----------|---------------------|             |-----------|---------------------|
                  |------------|------------|              |------------|------------|                         
                  |   синий    |   синий    |   
                  |------------|------------|    


По смыслу, условие HAVING эквивалентно условию WHERE, с той лишь разницей, что в условии мы уже можем использовать агрегатные функции, поскольку условие применяется к результату группировки. Кто не в курсе про HAVING, может добиться 
того же результата "надстройкой" еще одного уровня вложенности запроса с WHERE, но зачем писать больше?..


SELECT                                                SELECT
    str                                                        *
,   sun(i)                                               FROM (
FROM (                                                   SELECT
    VALUES                                                   str
        ('a', 1), ('b', 2)                                  ,   sum(i)
    ,   ('a', 3), ('b', 4)                                  FROM ...
) T(str, i)                                                   GROUP BY
GROUP BY                                                  1
    1                                                       ) T
HAVING                                                WHERE
    sum(i) > 5;                                          sum > 5;
-------------------------------------------------------------------    HAVING заменяет "надзапрос" с WHERE
str  | sum
text | integer
   b |       6


Итого, в этой лекции мы разобрали следующие возможности команды SELECT, и это оказалось все-таки не так уж сложно!


SELECT [ DISTINCT [ ON ( выражение [, ...] ) ] ]
    [ * | выражение [ [ AS ] имя_результата ] [, ...] ]
    [ FROM элемент_FROM [, ...] ]
    [ WHERE условие ]
    [ GROUP BY элемент_группирования [, ...] ]
    [ HAVING условие ]
    [ ORDER BY выражение
        [ ASC | DESC | USING оператор ]
        [ NULLS { FIRST | LAST } ] [, ...]
    ]
    [ LIMIT { число | ALL } ]
    [ OFFSET начало [ ROW | ROWS ] ]
    [ FETCH { FIRST | NEXT } [ число ] { ROW | ROWS } { ONLY | WITH TIES } ]


А про более сложные варианты использования SELECT мы поговорим на следующей лекции.





______________________________________________________________________________________
###################################################################
______________________________________________________________________________________
Сложные SELECT

В этой лекции углубимся в расширенные возможности команды SELECT : как можно "сложить" и "вычесть" выборки (UNION/INTERSECT/EXCEPT), или запомнить их и использовать повторно (даже в рекурсивных запросах), что дают оконные 
функции (WINDOW) и соединения (JOIN).



Операции над множествами
По сути, любая выборка, которую мы научились создавать и как-то минимально обрабатывать на прошлой лекции, представляет из себя некоторое множество записей. Поэтому сегодняшнюю лекцию мы и начнем с разбора операций, которые над этими 
множествами можно совершать. Стандарт SQL допускает 3 таких операции: объединение (UNION), пересечение (INTERSECT) и исключение (EXCEPT). 


#############
UNION
Самой простой из них, конечно, является операция объединения:

оператор_SELECT UNION [ ALL | DISTINCT ] оператор_SELECT


	               UNION ALL							            UNION
	|------------| 		  |------------|				|------------| 		     |------------|
	|  красный   |		  |  красный   |				|  красный   |		     |  красный   |
	|------------| =========> |------------|				|------------| =========>    |------------|
	|   жёлтый   |            |   жёлтый   |				|   жёлтый   |		     |   жёлтый   |
	|------------|      ------|------------|------- 			|------------|         ------|------------|-------
				  |  зелёный   |      					           //====>   |  зелёный   |
				  |------------|					           ||  	     |------------| 
	|------------|	  //===>  |   жёлтый   |				|------------|     ||
	|  зелёный   |	  ||	  |------------|				|  зелёный   |     ||
	|------------|    ||	  |  красный   |				|------------|     ||
	|   жёлтый   |====//	  |------------|				|   жёлтый   | ====//
	|------------| 							        |------------| 
	|  красный   |							        |  красный   |
	|------------|							        |------------|




Ничего сложного в ее использовании в запросе нет: "SELECT слева - SELECT справа", если верить документации. Но на практике, обычно, эти SELECT оказываются не "слева-справа", а "сверху-снизу" от ключевого слова UNION - так гораздо 
удобнее графически воспринимать сложные запросы:

  VALUES
    (1, 2)
  , (1, 2)
UNION
  VALUES
    (3, 4)
  , (1, 2);


###
UNION ALL vs UNION [DISTINCT]

Как видно из описания, существует две формы объединения:
- UNION ALL, которая просто "склеивает" две выборки, никак не вмешиваясь в их содержимое. Тут можно заметить, что UNION ALL всегда в PostgreSQL выводит сначала записи первой выборки, а потом второй, причем никак не изменяя их порядка.
Такое поведение не описано в стандарте, а реализовано by design, что открывает возможности для оптимизации выполнения запросов с использованием UNION ALL + LIMIT.

- UNION "просто" (ключевое слово DISTINCT тут можно безболезненно опустить), который "уникализирует" записи результирующей выборки - фактически X UNION Y, это "синтаксический сахар" для

SELECT DISTINCT
  *
FROM
  (
    X
  UNION ALL
    Y
  ) T;


Тут важно понимать, что "уникализация" производится для всей результирующей выборки, а не только для второго блока. Например, в этом примере запись (1, 2), повторяющаяся уже в первой выборке, останется в единственном экземпляре:


		UNION vs UNION ALL

  VALUES                                                     VALUES
    (1, 2)                                                   	(1, 2)
  , (1, 2)                                                    , (1, 2)
UNUON                                                        UNION ALL
  VALUES                                                     	VALUES    
    (3, 4)                                                        (3, 4)
  , (1, 2);                                                     , (1, 2);
-----------------------                                 ------------------------------
column1 | column2                                          column1 | column2
integer | integer                                          integer | integer
      1 |       2                                                1 |       2
      3 |       4                                                1 |       2
                                                                 3 |       4
                                                                 1 |       2


X UNION Y == X UNION DISTINCT Y == DISTINCT (X UNION ALL Y)


При объединении выборок, как и при любой другой операции над множествами, важно помнить три момента.
Первый касается тех разработчиков, которые любят "клеить" текст запроса где-то в коде приложения - это неправильная "свертка" nil/null/undefined-значений, которая допускает пропуск части столбцов. В этом случае вы рискуете получить 
разное количество столбцов в объединяемых выборках, что приведет к ошибке:

ERROR:  each UNION query must have the same number of columns

Но даже если вы не пропустили NULL-значение, а сгенерировали текст запроса корректно, это вовсе не означает, что он отработает без ошибки - ведь "нетипизованный" NULL по умолчанию приведется к типу text, а типы каждой пары столбцов 
объединяемых выборок должны совпадать, иначе...

ERROR:  UNION types integer and text cannot be matched

Но даже если вы правильно указали тип каждого NULL, но использовали UNION-форму (без ALL) - опять можете получить сюрприз, поскольку все NULL-значения одинаковы при уникализации, хотя в точке генерации это могли быть разные 
значения - например, null и undefined.

  VALUES
    (1, 2)
UNION
  VALUES
    (3);
---------------
ERROR: each UNION query must have the SAME number of columns

 

  VALUES
    (1, 2)
UNION
  VALUES
    (3, NULL);
---------------
ERROR: UNION types integer and text cannot be matched



  VALUES
    (1, 2)
UNION
  VALUES
    (3, NULL::integer)
  , (3, NULL::integer);
---------------------------
column1 | column2
integer | integer
      1 |       2
      3 |              -- все NULL одинаковые!






###################
Пересечения (INTERSECT) и исключения (EXCEPT)

Раз есть объединение множеств, то должно же быть и пересечение и исключение. И оно есть, и описывается точно так же, как и UNION:

оператор_SELECT INTERSECT [ ALL | DISTINCT ] оператор_SELECT

оператор_SELECT EXCEPT [ ALL | DISTINCT ] оператор_SELECT



						Пересечение и исключение

	
	                INTERSECT							        EXCEPT
	|------------| 		   |------------|				|------------| 		
	|  красный   |		   |  красный   |				|  красный   |		   |------------|
	|------------| ==========> |------------|				|------------| ==========> |  зелёный   |
	|   жёлтый   |      ||	   |   жёлтый   |				|   жёлтый   |	   ||	   |------------|
	|------------|      ||     |------------|				|------------|     ||      
	|  зелёный   |      ||				                        |  зелёный   |	   ||
	|------------|	    ||						        |------------| 	   ||
		 	    ||								           ||
	|------------|      ||						        |------------| 	   ||
	|   жёлтый   |======//					                |   жёлтый   |     ||
	|------------| 							        |------------| ====//
	|  красный   |							        |  красный   |
	|------------|							        |------------| 



Разница разве что в вычислении количества "совпадающих" записей при ALL-операции:

-- m - экземпляров записи в первой выборке, n - во второй
UNION ALL     -> sum(m, n)
INTERSECT ALL -> min(m, n)
EXCEPT ALL    -> max(m - n, 0)

То есть пересечение (INTERSECT) оставляет только те записи, которые присутствуют и в первой, и во второй выборке, а исключение (EXCEPT) - только те, которые есть в первой, но не во второй. При этом UNION, INTERSECT и EXCEPT могут 
произвольным образом комбинироваться в одном запросе. Чтобы понять порядок их вычисления, проще всего соотнести их с арифметическими операциями:

UNION     ->  +  -> low          A UNION B INTERSECT C EXCEPT D
EXCEPT    ->  -  -> low          A + B * C - D
INTERSECT ->  *  -> high         A UNION (B INTERSECT C) EXCEPT D





###################
Common Table Expression, CTE (WITH)

Мы уже научились выборки создавать, объединять, пересекать, то надо же их как-то научиться запоминать для возможности повторного использования. Для этого в SQL используется ключевое слово WITH, которое реализует функционал обобщенных 
табличных выражений:

[ WITH [ RECURSIVE ] запрос_WITH [, ...] ]
  имя_CTE [ (имя_столбца, ...) ] AS (                  -- Common Table Expression
    { SELECT | TABLE | VALUES | 
      { INSERT | UPDATE | DELETE } ... RETURNING ...
    } |
    {
      нерекурсивная_часть
    UNION [ ALL | DISTINCT ]
      рекурсивная_часть
    }
)


Такой запоминаемой под определенным именем выборкой может быть результат как "обычных" SELECT, TABLE или VALUES, так и INSERT, UPDATE или DELETE (с PostgreSQL 16 еще добавился MERGE) в RETURNING-форме. При этом имена столбцам выборки 
можно задавать как "внутри", так и "снаружи":


WITH v AS (                                                         
  VALUES 
    (1, 2)
)
  TABLE v
UNION ALL
  TABLE v;
---------------
column1 | column2
integer | integer
      1 |       2
      1 |       2




WITH v(x, y) AS (
  SELCET
    1 a
  , 2 b
)
  TABLE v
UNION ALL
  TABLE v;
---------------------
x       | y
integer | integer
      1 |       2
      1 |       2



Изнутри генерирующего запроса можно ссылаться на уже описанные выше по тексту CTE, то есть можно получить своеобразную суперпозицию g(f(x)):

WITH f AS (
  TABLE x -- это обращение к реальной таблице
)
, g AS (
  TABLE f -- это уже обращение к сформированной CTE
)
TABLE g;




###################
Рекурсивные CTE (WITH RECURSIVE)

Другой вариант описания CTE - рекурсивный (в этом случае к WITH необходимо добавить ключевое слово RECURSIVE), когда генерирующий выборку запрос может ссылаться не только на предыдущие CTE, но и "на самого себя":


WITH RECURSIVE fib(i, a, b) AS (
  VALUES(0, 0, 1)  -- затравка
UINON ALL
  SELECT               -- шаг рекурсии
    i + 1
  , greatest(a, b)
  , a + b
  FROM
    fib                      -- обращение к себе
  WHERE
    i < 10                -- условие продолжения
) 
TABLE fib;
-----------------------------------------------------	(Рекурсивная генерация последовательности Фибоначчи)
i       | a       | b
integer | integer | integer   
      0 |       0 |       1
      1 |       1 |       1
      2 |       1 |       2
      3 |       2 |       3
      4 |       3 |       5
      5 |       5 |       8
      6 |       8 |      13
      7 |      13 |      21
      8 |      21 |      34
      9 |      34 |      55
     10 |      55 |      89 



В этом случае он состоит из нерекурсивной выборки-"затравки", оператора UNION ALL (или просто UNION, но его внутри рекурсии стоит использовать с осторожностью) и шага рекурсии с условием продолжения - под ним. На каждом следующем шаге 
рекурсии такой запрос получает "на вход" (под именем "своей" CTE) результат генерации записей предыдущего сегмента, пока этот результат непустой, или затравочную выборку - для первого шага:

	WITH RECURSIVE
 	|------------|
	|  красный   | <===\\
        |------------|     ||
		           ||
	|------------|  ===//                      
        |  жёлтый    | 
        |  зелёный   | =====\\
        |------------|      ||
                            ||
	|--------------|    ||
        | голубой      | ===//
 	| синий        |
	| фиолетовый   |
	|--------------|


Важно понимать, что хоть какое-то условие (по наличию записей, их количеству, счетчику шагов или времени выполнения) должно ограничивать продолжение формирования выборки, иначе есть риск получить бесконечно выполняющийся запрос. Потому 
что если мы забудем где-то ограничить нашу рекурсию, то можем очень быстро вспомнить легенду о зернах на клетках шахматной доски, потому что нет никаких ограничений на количество записей в сегменте, и оно легко может расти 
экспоненциально, запросто поглощая любой доступный объем памяти на сервере:

WITH RECURSIVE exp(i, n) AS (
  VALUES(0, 1)
UNION ALL
  SELECT
    i + 1
  , unnest(ARRAY[n * 2, n * 2])
  FROM 
    exp
  WHERE
    i < 2
)
TABLE exp;
------------------------------------------- Удвоение количества записей на каждом шаге рекурсии
i       | n 
integer | integer
      0 |       1
      1 |       2
      1 |       2
      2 |       4
      2 |       4
      2 |       4 
      2 |       4





###################
Оконные функции (WINDOW / OVER)

Следующая полезная "фича" PostgreSQL заключается в поддержке возможностей оконных функций - выполняемых по глобально определенному в рамках SELECT-запроса с помощью WINDOW-блока "окну" или по определенному локально с помощью
ключевого слова OVER:

WINDOW имя_окна AS ( определение_окна ) [, ...]
  [ имя_существующего_окна ]
  [ PARTITION BY выражение [, ...] ]
  [ ORDER BY выражение [ ASC | DESC | USING оператор ] [ NULLS { FIRST | LAST } ] [, ...] ]
  [ предложение_рамки ]
рамка:
  { RANGE | ROWS | GROUPS } начало_рамки [ исключение_рамки ]
  { RANGE | ROWS | GROUPS } BETWEEN начало_рамки AND конец_рамки [ исключение_рамки ]					-- Описание "окна": сегментация, сортировка и рамка
  начало/конец                           исключение
  UNBOUNDED PRECEDING    EXCLUDE CURRENT ROW
  смещение PRECEDING          EXCLUDE GROUP
  CURRENT ROW                      EXCLUDE TIES
  смещение FOLLOWING        EXCLUDE NO OTHERS
  UNBOUNDED FOLLOWING


По сути, оконные функции являются гибридом группировки и рекурсии - с одной стороны, они позволяют весь набор записей разделить на группы (PARTITION BY) и работать с каждым изолированно, с другой - последовательно накапливать данные 
или "заглядывать" в предыдущие значения, если задан их порядок (ORDER BY) на определенное количество записей "вперед/назад" (RANGE/ROWS/GROUPS).



###
Оконные функции вместо рекурсии

Следующий пример демонстрирует возможность замены рекурсивного вычисления последовательности использованием оконных функций для вычисления "треугольных" чисел:



                                        -- Вычисление "треугольных" чисел через рекурсию и оконные функции

WITH RECURSIVE sum(i, s) AS (                                               SELECT
  VALUES(1, 1)                                                                i
UNION ALL                                                                   , sum(i) OVER(ORDER BY i) s -- "окно"
  SELECT                                                                    FROM
    i + 1                                                                     generate_series(1, 10) i;
  , s + (i + 1)
  FROM
    sum
  WHERE
    i < 10
)
TABLE sum;
----------------------------------------------------------------------------------------------------------------------------------------------
1 = 1 
3 = 1 + 2
6 = 1 + 2 + 3
10 = 1 + 2 + 3 +4


Здесь в определении локального "окна" (в OVER-блоке) мы указали только лишь порядок (ORDER BY) обработки записей. Это означает, что все записи выборки будут отнесены в одну группу и для каждой записи функция (в нашем случае - sum) 
вычисляется на множестве записей от первой до текущей:


					Накопительный сумматор

	  i      ORDER BY     agg  sum(i)
	|---|		          |---|
	| 1 | =||=||=||=||=====>  | 1 |			OVER(ORDER BY i)
	|---|  \/ || || ||	  |---|			  ==
	| 2 |   ==||=||=||=====>  | 3 |                 OVER(ORDER BY i
	|---|     \/ || ||        |---|                   ROWS BETWEEN
  	| 3 |      ==||=||=====>  | 6 |                     UNBOUNDED PRECEDING AND
 	|---|        \/ ||        |---|                     CURRENT ROW)
	| 4 |         ==||=====>  | 10|
 	|---|	        \/        |---|
	| 5 |	         ======>  | 15|
	|---|		          |---|
	| 6 |		          | 21|
	|---| 		          |---|
 	| 7 | 		          | 28|
	|---| 		          |---|
	| 8 | 		          | 36|
	|---| 		          |---|
	| 9 |		          | 45|
	|---|		          |---|
	| 10|		          | 55|	
	|---|		          |---|




То есть использование OVER(ORDER BY i) эквивалентно вот такому определению "рамки":

OVER(
  ORDER BY i                -- определение порядка записей
  ROWS BETWEEN              -- ключевые слова "рамки":
    UNBOUNDED PRECEDING AND -- ... от самого начала
    CURRENT ROW             -- ... до текущей записи
)


А вот без заданной сортировки OVER() использует по умолчанию совсем другую "рамку", что может вызвать проблемы у неопытного разработчика:

OVER(
  ROWS BETWEEN
    UNBOUNDED PRECEDING AND -- ... от самого начала
    UNBOUNDED FOLLOWING     -- ... до самого конца
)



                                          Глобальный сумматор

SELECT				OVER()
  i				  ==
, sum(i) OVER s			OVER(
FROM				  ROWS BETWEEN
  generate_series(1, 5) i;	    UNBOUNDED PRECEDING AND
--------------------------------    UNBOUNDED FOLLOWING)
i       | s
integer | bigint
      1 |     55
      2 |     55
      3 |     55
      4 |     55
      5 |     55


Более полно с возможностями определений "рамки" можно ознакомиться в документации. (https://postgrespro.ru/docs/postgresql/16/sql-expressions#SYNTAX-WINDOW-FUNCTIONS)




###################
Совместное вычисление по разным "окнам"

В отличие от группировки, которая всю выборку делит на "кучки" по единому правилу, оконные функции могут использовать каждая свои правила сегментирования и даже доопределять параметры "окна", описанного в запросе глобально:

SELECT
  i
, row_number() OVER w r
, sum(i) OVER(w
    ROWS BETWEEN 1 PRECEDING
    AND 1 FOLLOWING -- скользящее окно
    EXCLUDE CURRENT ROW
  ) s
FROM
  generate_series(0, 9) i
WINDOW -- тут все окна
  w AS (PARTITION BY i / 5 ORDER BY i);
------------------------------------------------------                    Одно окно, разные "рамки"
i       | r      | s 
integer | bigint | bigint
      0 |      1 |      1  -- i / 5 ==0
      1 |      2 |      2
      2 |      3 |      4  -- 1(p) + 3(f)
      3 |      4 |      6
      4 |      5 |      3
      5 |      1 |      6  -- i / 5 == 1
      6 |      2 |     12
      7 |      3 |     14
      8 |      4 |     16
      9 |      5 |      8  -- 8(p) + NULL(f)



В этом примере мы разбили всю выборку по целочисленному результату от деления i на 5 (если мы делим в SQL целое на целое, то и результатом будет целое) и отсортировали по исходному значению. Затем, с помощью функции row_number мы 
пронумеровали, начиная с 1, каждую запись внутри каждой группы, а вот сумму мы вычислили по "рамке" от предыдущей строки до следующей, исключив текущую (EXCLUDE). Такой подход может быть полезен, например, если вам надо заполнить 
пропуски на основании соседних данных.



###################
Оконные функции вместо группировки

Использование оконных функций хоть и похоже на использование агрегатных функций при группировке, но дает значительно больше возможностей. Если при группировке исходные данные теряются (группы "схлапываются" до единственной строки 
результата), а правила сегментирования должны быть едины для всех агрегатов, то оконные функции и исходные данные не трогают, и правила могут быть свои для каждой:


								Группировка vs оконные функции


		           FROM                                         GROUP BY 						    FROM				  OVER
	/-      |------------|------------|               |-----------|-------------------|		         /-     |------------|------------|	       |------------|---------------------|
	|       |  красный   |  красный   |   ========>   |  красный  | красно-жёлтый     |			 |      |  красный   |  красный   | ========>  |  красный   |   тускло-красный    |
       <        |------------|------------|               |-----------|-------------------|			<       |------------|------------|	       |------------|---------------------|
	|       |  красный   |  жёлтый    | 									 |      |  красный   |  жёлтый    |            |  красный   |   тускло-жёлтый     | 
  /-    \-      |------------|------------|               |-----------|--------------------|		  /-     \-     |------------|------------|            |------------|---------------------|
  |             |  зелёный   |  зелёный   |  ========>    |  зелёный  | зелёно-голубой     |              |             |  зелёный   |  зелёный   | ========>  |  зелёный   |   тускло-зелёный    |
 <	        |------------|------------|               |-----------|--------------------|		 <	        |------------|------------|            |------------|---------------------|
  |             |  зелёный   |  голубой   |								  |             |  зелёный   |  голубой   |	       |  зелёный   |   тускло-голубой    |
  \-     /-     |------------|------------|               |------------|------------|  			  \-     /-     |------------|------------|            |------------|---------------------|
        <       |   синий    |   синий    |  ========>    |   синий    |   синий    |                   	<       |   синий    |   синий    |  ========> |   синий    |    тускло-синий     |            
         \-     |------------|------------|               |------------|------------| 				 \-     |------------|------------|            |------------|---------------------|




Впрочем, с помощью оконных функций можно достичь того же результата, что и при группировке - достаточно использовать DISTINCT ON по всему набору ключей сегментирования (PARTITION BY):


			Оконные функции могут заменить группировку

SELECT DISTINCT ON(i / 5) -- сегмент			SELECT
  i / 5							  i / 5 -- ключ группировки
, sum(i) OVER(PARTITION BY i / 5) s			, sum(i) s
FROM							FROM
  generate_series(0, 9) i;				  generate_series(0, 9) i
							GROUP BY
							  1;
-------------------------------------------------------------------------------------------------------------------------------
			?column? | s
			integer  | bigint
			       0 |     10
			       1 |     35





###################
Расширенные возможности агрегатных функций (GROUP BY)

Раз уж мы снова вспомнили про группировку данных, то используемые при ней агрегатные функции имеют существенно больше возможностей, чем мы рассмотрели в прошлой лекции:


		Агрегатные функции с уникализацией, упорядочением, фильтрацией и сортирующие

агрегатная_функция ([ ALL | DISTINCT ] выражение [, ...]
  [ ORDER BY предложение_order_by ] )
  [ FILTER ( WHERE условие_фильтра ) ]

агрегатная_функция ( * )
  [ FILTER ( WHERE условие_фильтра ) ]

агрегатная_функция ( [ выражение [, ...] ] ) WITHIN GROUP (ORDER BY предложение_order_by)
  [ FILTER ( WHERE условие_фильтра ) ]


На примере функции string_agg, склеивающей текстовые строки в одну, мы можем увидеть, как работает сортировка (ORDER BY), уникализация значений (DISTINCT) и фильтрация (FILTER) строк:

SELECT
  string_agg(i::text, ',') soa
, string_agg(i::text, ',' ORDER BY i DESC) sod -- сортировка всегда в конце
, string_agg(i::text, ',') FILTER(WHERE i % 2 =0) sf
, string_agg(DISTINCT (i % 3)::text, ',') sd
FROM
  generate_series(0, 9) i;
-------------------------------------------------------------------------------------------                   Разные варианты применения string_agg





###################
Сортирующие и гипотезирующие функции (WITHIN GROUP)

Но если для обычных агрегирующих функций сортировка является дополнительной возможностью, то для некоторых, определяющих значение на определенном месте выборки или порядковое место, соответствующее значению, задание этого самого 
порядка обязательно в блоке WITHIN GROUP(ORDER BY ...). В настоящий момент PostgreSQL поддерживает 3 сортирующие функции:

1) mode() WITHIN GROUP ( ORDER BY anyelement ) -> anyelement
Вычисляет моду - наиболее часто встречаюзееся в агрегируемом аргументе значение (если одинаково часто встречаются несколько значений, произвольно выбирается первое из них). Агрегируемый аргумент должен быть сортируемого типа

2) 
percentile_cont(fraction double presision) WITHIN GROUP (ORDER BY double precision) -> double precision
percentile_cont(fraction double presision) WITHIN GROUP (ORDER BY interval) -> interval
Вычисляет непрерывный процентиль - значение, соответствующее дроби, заданной параметром fraction, в отсортированном множестве значений агрегатного аргумента. При этом в случае необходимости соседние входные значения будут 
интерполироваться.

percentile_cont(fraction double presision[]) WITHIN GROUP (ORDER BY double precision) -> double precision[]
percentile_cont(fraction double presision[]) WITHIN GROUP (ORDER BY interval) -> interval[]
Вычисляет множественные непрерывные процентили. Возвращает массив той же размерности, что имеет параметр fraction, в котором каждый отличный от NULL элемент заменяется соответствующим данному перцентилю значением (возможно
интерполированным).

3)
percentile_disc(fraction double presision) WITHIN GROUP (ORDER BY anyelement) -> anyelement
Вычисляет дискретный процентиль - первое значение в отсортированном множестве значений агрегатного аргумента, позиция которого в этом множестве равна или больше значения fraction. Агрегируемый аргумент должен быть сортируемого типа.

percentile_disc(fraction double presision[]) WITHIN GROUP (ORDER BY anyelement) -> anyarray[]
Вычисляет множественные дискретные процентили. Возвращает массив той же размерности, что имеет параметр fraction, в котором каждый отличный от NULL элемент заменяется соответствующим данному перцентилю значением. Агрегируемый аргумент
должен быть сортируемого типа.


... и 4 гипотезирующие функции:

1) rank (аргументы) WITHIN GROUP ( ORDER BY сортирующие_аргументты ) -> bigint
Вычисляет ранг гипотетической строки с пропусками, т.е. номер первой родственной ей строки.

2) dense_rank( args ) WITHIN GROUP ( ORDER BY сортируемые_аргументы ) -> bigint
Вычисляет ранг гипотетической строки без пропусков; по сути эта функция считает группы родственных строк.

3) percent_rank( аргументы ) WITHIN GROUP ( ORDER BY сортируемые_аргументы ) -> double precision
Вычисляет относительный ранг гипотетической строки, т.е. (rank - 1) / (общее число строк - 1). Т.о, результат лежит в интервале от 0 до 1, включая границы.

4) cume_dist( аргументы ) WITHIN GROUP ( ORDER BY сортируемые_аргументы ) -> double precision
Вычисляет кумулятивное распределение, т.е. (число строк, предшествующих или родственных гипотетической строке) / (общее число строк). Результат лежит в интервале от 1/N до 1. 


SELECT
  percentile_cont(ARRAY[0.5, 0.9, 0.95])
    WITHIN GROUP(ORDER BY i)
, rank(1.5)
    WITHIN GROUP(ORDER BY i)
FROM
  generate_serues(0, 10) i;
--------------------------------------------------		Определение значений нескольких персентилей сразу и ранга конкретного значения
percentile_cont    | rank
double precision[] | bigint
{5,9,9.5}          |      3




###################
Наборы группирования (GROUPING SETS)

Еще одна полезная возможность - группировка одной и той же исходной выборки сразу по нескольким комбинациям ключей - наборам группирования (GROUPING SETS). одробно в рамках этой лекции останавливаться на ней не буду, поскольку о ее 
пользе можно почитать в моей недавней статье SQL HowTo: итоги по строкам и столбцам «в одно действие» (https://habr.com/ru/post/781926/).



###################
Функции в исходной выборке (FROM)

Мы уже выяснили, что источник данных для обработки должен находиться во FROM-блоке SELECT-запроса. Мы уже попробовали варианты, когда там используется реальная таблица базы, выборка как результат вложенного запроса, CTE и даже 
генерирующая функция. И прежде чем затронуть наиболее сложную тему соединений, немного отвлечемся как раз на функции.

###
Нумерация строк результата функции (WITH ORDINALITY)

В одном из предыдущих примеров мы использовали функцию generate_series для формирования выборки из арифметической последовательности значений, а оконную функцию row_number - для порядковой нумерации этих самых полученных значений.
Но именно для этой задачи такой подход является избыточным - можно просто добавить столбец с порядковым номером с помощью ключевого слова WITH ORDINALITY:

SELECT					SELECT					SELECT
  *					  *					  *
FROM					FROM					, row_number() OVER() ord
  generate_series(1, 10, 2)			  generate_series(1, 10, 2) 			FROM
    WITH ORDINALITY T(i, ord);			    WITH ORDINALITY;			  generate_series(1, 10, 2) i;
-----------------------------------------------------------------------------------------------------------------------------------        WITH ORDINALITY вместо row_number
				generate_series | ordinality
				integer         | bigint
					      1 |          1
					      3 |          2
 				              5 |          3
					      7 |          4
					      9 |          5


При необходимости можно сразу же переименовать столбцы результата, включая и порядковый номер. Если этого не сделать, его имя будет назначено автоматически как ordinality.





______________________________________________________________________________________
Соединения (JOIN) и выражения подзапросов

Наконец, мы добрались до самой сложной, но и самой полезной функции SQL - соединения выборок (не путать с объединением!):

соединение                                                                                                            -- Виды соединений
  элемент_FROM, элемент_FROM
  элемент_FROM, CROSS JOIN элемент_FROM
  элемент_FROM {
    [ INNER ]
    | LEFT [ OUTER ]
    | RIGHT [ OUTER ]
    | FULL [ OUTER ]
  } JOIN элемент_FROM
    { ON условие_соединения | USING ( столбец_соединения [, ...] ) [ AS псевдоним_использования_соединения ] }
элемент_FROM NATURAL { [ INNER ] | LEFT | RIGHT | FULL } JOIN элемент_FROM


Фактически, соединение говорит базе как сопоставить записи выборок слева и справа.


###################
CROSS JOIN

И самый простой способ такого сопоставления - перекрестное соединение, оно же декартово произведение множеств (ага, снова множества), оно же прямое произведение, оно же CROSS JOIN, оно же "через запятую". Каждая пара строк из 
левой (X) и правой (Y) выборок образуют X * Y строк результата, состоящих из cX + cY столбцов:

SELECT
  *
FROM
  (
    VALUES
      (1, 2)
    , (3, 4)
    , (5, 6)
  ) X(a, b)
CROSS JOIN -- или "через запятую" - X, Y
  (
    VALUES
      (1, 8)
    , (3, 10)
    , (3, 12)
    , (7, 14)
  ) Y(a, c)
------------------------------------------	CROSS JOIN
a       | b       | a       | c
integer | integer | integer | integer
      1 |       2 |       1 |       8
      1 |       2 |       3 |      10
      1 |       2 |       3 |      12
      1 |       2 |       7 |      14
      3 |       4 |       1 |       8
      3 |       4 |       3 |      10
      3 |       4 |       3 |      12
      3 |       4 |       7 |      14
      5 |       6 |       1 |       8
      5 |       6 |       3 |      10
      5 |       6 |       3 |      12
      5 |       6 |       7 |      14


Причем некоторые из имен столбцов могут даже задублироваться, содержа при этом разные значения в одной и той же записи - поэтому SELECT * не стоит использовать при таком соединении.



###################
JOIN / INNER JOIN

От самого простого перекрестного соединения перейдем к наиболее типичному - внутреннему. При нем сопоставляются только те пары записей, для которых выполняется условие:

SELECT
  *
FROM (
    VALUES
      (1, 2)
    , (3, 4)
    , (5, 6)
  ) X(a, b)
JOIN            -- или INNER JOIN
  (
    VALUES
      (1, 8)
    , (3, 10)
    , (3, 12)
    , (7, 14)
  ) Y(a, c)
    USING(a);   -- или ON-условие
-----------------------------------       	INNER JOIN
a       | b       | c
integer | integer | integer
      1 |       2 |       8 -- (1,2) x (1,8)
      3 |       4 |      10 -- (3,4) x (3,10)
      3 |       4 |      12 -- (3,4) x (3,12)
---------------------------------------------------------
X INNER JOIN Y ON cond == X CROSS JOIN Y WHERE cond
X INNER JOIN Y USING(a) == X CROSS JOIN Y WHERE X.a = Y.a
X INNER JOIN Y ON TRUE == X CROSS JOIN Y


По сути, INNER JOIN (впрочем, ключевое слово INNER в PostgreSQL можно опустить) можно рассматривать как результат CROSS JOIN с последующим наложением ON-условия:

X CROSS JOIN Y WHERE cond
->
X INNER JOIN Y ON cond




###################
Подводные камни соединений

Во-первых, конечно же, NULL-значения! Если они окажутся в сопоставляемых полях, то условие от них даст тоже NULL-результат, и такие строки не будут сопоставлены.
Во-вторых, обратите внимание, что в нашем примере строка со значением a = 3 слева была в одном экземпляре, а справа таких строк было две - поэтому и в результате они у нас "умножились". А вот запись со значением a = 5 не нашла себе 
пару справа, и из результата исчезла.




###################
LEFT JOIN

А чтобы все-таки сохранить такую строку "без пары" в результирующей выборке, существует левое соединение (оно же "левое внешнее", LEFT OUTER JOIN, но OUTER тоже принято опускать). В типичной модели использования условие обычно 
формируется на основе совпадения значений одноименных столбцов обеих выборок. Чтобы не писать такие условия многократно, можно использовать USING-форму:

SELECT
  *
FROM
  (
    VALUES
      (1, 2)
    , (3, 4)
    , (5, 6)
  ) X(a, b)
LEFT JOIN -- или LEFT OUTER JOIN
  (
    VALUES
      (1, 8)
    , (3, 10)
    , (3, 12)
    , (7, 14)
  ) Y(a, c)
    USING(a);   -- или ON-условие
--------------------------------------------             LEFT JOIN
a       | b       | c
integer | integer | integer
      1 |       2 |       8   -- (1,2) x (1,8)
      3 |       4 |      10   --  (3,4) x (3,10)
      3 |       4 |      12   --  (3,4) x (3,12)
      5 |       6 |           -- (5,6) x (NULL) 
--------------------------------------------------------
X LEFT JOIN Y ON cond
  WHERE Y IS DISTINCT FROM NULL
  ==
X INNER JOIN Y ON cond


Это ровно то же самое внутреннее соединение, к результатам которого мы добавили не нашедшие себе пару строки из левой выборки, заполнив остальные столбцы результата NULL-значениями.




###################
RIGHT JOIN

Симметрично левому, существует и правое соединение, оставляющие строки из правой выборки. Впрочем, они взаимозаменяемы с LEFT JOIN при перестановке выборок (антикоммутативны), потому RIGHT JOIN является не более 
чем "синтаксическим сахаром":

X RIGHT JOIN Y ON cond
->
Y LEFT JOIN X ON cond



###################
FULL JOIN

При LEFT JOIN мы добавляли "непарные" строки из левой выборки, при RIGHT JOIN - из правой, а если добавить и те, и другие - получится полное соединение. Проще всего представить разные типы соединений в виде графической схемы:
(https://habr.com/ru/companies/tensor/articles/785144/    рисунок 32)

										Схема типов соединений: INNER JOIN в ядре каждого результата
-------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------
   IJ      CJ		               \ красный		  \ жёлтый			\ синий      		 |	RJ	 	               \ красный          \ жёлтый		\ синий	
	красный \		красный\красный		   красный\жёлтый	         красный\синий      		 |	красный \		красный\красный
	красный \ 	        красный\красный		   красный\жёлтый                красный\синий       		 |	красный \		красный\красный
	зелёный \		зелёный\красный		   зелёный\жёлтый                зелёный\синий     		 |	зелёный \
											 		                 |	белый\				             белый\жёлтый          белый\синий	
-------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------
	LJ		               \ красный		  \ жёлтый			\ синий		\белый   |	FJ			       \ красный	  \ жёлтый		 \ синий		    \белый
	красный \               красный\красный                                                                          |	красный \		красный\красный
	красный \               красный\красный                                                                          |	красный \		красный\красный
	зелёный \									                 зелёный\белый	 |	зелёный \									             зелёный\белый
													                 |	белый\				             белый\жёлтый           белый\синий
-------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------


Но если у кого-то вдруг сложилось ощущение, что условие может быть только по равенству значений, то это не так - например, можно сопоставить записи по условию <=:


SELECT
  *
FROM
  (
    VALUES
      (1, 1)
    , (2, 2)
    , (3, 3)
  ) X(a, b)
INNER JOIN
  (
    VALUES
      (1, 1)
    , (2, 2)
    , (3, 3)
  ) Y(a, c)
    ON X.a <= Y.a;
-----------------------			Соединение по <=
a       | b       | c
integer | integer | integer
      1 |       1 |       1
      1 |       1 |       2
      1 |       1 |       3
      2 |       2 |       2
      2 |       2 |       3 
      3 |       3 |       3





###################
Автоформирование условия по столбцам (USING, NATURAL)

В типичной модели использования условие сопоставления обычно формируется на основе совпадения значений одноименных столбцов обеих выборок. Чтобы не писать такие условия многократно, можно использовать USING-форму:

X ??? JOIN Y ON Y.a = X.a AND Y.b = X.b
->
X ??? JOIN Y USING(a, b)

При этом упомянутые в USING столбцы будут присутствовать в результате заведомо однократно, в отличие от ON-условия. Если же нам необходимо установить соответствие по значениям всех одноименных столбцов, можно написать еще 
меньше - в NATURAL-форме, без перечисления всех этих столбцов:

X(a, b, x) ??? JOIN Y(a, b, y) USING(a, b)  -> (a, b, x, y)
->
X(a, b, x) NATURAL ??? JOIN Y(a, b, y)      -> (a, b, x, y)





______________________________________________________________________________________
Порядок вычисления выборок (LATERAL)

Как мы видели на прошлой лекции, порядок вычислений что выражений, что выборок, стандарт SQL оставляет на волю внутренней реализации СУБД - поэтому обратиться из одной выборки во FROM к "предыдущей" нельзя.

SELECT							SELECT
  *							  *
FROM							FROM
  (							  (
    VALUES(1)						    VALUES(1)
  ) X(i)                                                  ) X(i)
, ( 							, LATERAL (
    VALUES(i + 1)					    VALUES(i + 1)
  ) Y(j);						  ) Y (j);
----------------------					--------------------------------	Доступ к данным "предыдущей" выборки через LATERAL
ERROR: column "i" does not exist				i       | j
LINE 8:      VALUES(i + 1)					integer | integer
							              1 |       2


Единственным исключением является вызов функции во FROM - LATERAL перед ним писать можно, но бессмысленно - функции и так вычисляются в PostgreSQL после предыдущих выборок:

SELECT
  *
FROM
  (
    VALUES
      (2)
    , (3)
    , (5)
  ) T(i)
, generate_series(1, i) j;      -- тут мы успешно ссылаемся на данные предыдущей выборки




###################
Выражения подзапросов

Но, в общем-то, вложенный запрос может находиться и не во FROM-блоке, если нужен нам всего лишь для проверки какого-то условия, а не для получения данных из него. Поэтому в SQL есть набор операторов, проверяющих выражения для 
подзапросов.

###
EXISTS

Проверяет наличие хотя бы одной строки в результирующей выборке. Например, вместо INNER JOIN, умножащего записи с одинаковыми ключами, можно использовать EXISTS-проверку:

SELECT
  *
FROM
  X
WHERE
  EXISTS (
    SELECT
      NULL
    FROM
      Y
    WHERE
      a = X.a;
  );   



###
IN/NOT IN

Оператор IN позволяет проверить наличие значения во вложенной выборке (в этом случае она должна возвращать единственный столбец):

SELECT
  *
FROM
  (
    VALUES
      (1, 1)
    , (2, 2)
  ) X(a, b)
WHERE
  a IN (
    VALUES
      (1)
    , (3)
    );

Или можно сравнивать даже целые записи (тогда должно совпадать количество столбцов):

SELECT
  *
FROM
  (
    VALUES
      (1, 1)
    , (2, 2)
  ) X(a, b)
WHERE
  (a, b) IN (
    VALUES
      (1, 1)
    , (1, 2)
    , (2, 1)
    , (2, 2)
    );



###
ANY/ALL

Ключевые слова ANY и ALL позволяют вычислить истинность указанного оператора хотя бы для какой-то строки / для каждой строки вложенной выборки:

WITH Y AS (
  VALUES(1),(2)
)
SELECT
  *
, a < ANY(TABLE Y) -- хоть кто-то больше этого значения a?
, a < ALL(TABLE Y) -- все меньше этого значения a?
FROM
  (VALUES
    (1, 1)
  , (2, 2)
  ) X(a, b);

Если таким оператором выступает =, то = ANY эквивалентно работе оператора IN.




______________________________________________________________________________________
Управление порядком выполнения (CASE и coalesce )

Несмотря на невозможность "штатно" управлять ходом выполнения запроса, такая возможность все-таки существует с помощью оператора CASE и функции coalesce.

CASE
Оператор CASE может использоваться в двух вариантах:
- аналог цепочки if(...) ... else if(...) ... else if(...) .. else ... , когда на каждом шаге вычисляется значение нового выражения
- аналог switch (...) {case ...; case ...; default: ...}, когда значение оператора зависят от значений единственного выражения


###
CASE
  WHEN условие THEN результат
  [ WHEN ... THEN ... ]
  [ ELSE результат ]
END


CASE выражение
  WHEN значение THEN результат
  [ WHEN ... THEN ... ]
  [ ELSE результат ]
END

В обоих случаях CASE возвращает какое-то значение. Возможно, это будет NULL, если не произошло ни одного совпадения с WHEN-условием, а опциональный ELSE-блок не описан.

SELECT
  i
, CASE i % 2
    WHEN 0 THEN 'even'
    WHEN 1 THEN 'odd'
  END v1
, CASE
    WHEN i % 15 = 0 THEN 'foobar'
    WHEN i %   3 = 0 THEN 'foo'
    WHEN i %   5 = 0 THEN 'bar'
  END v2
FROM
  generate_series(0, 15) i;
------------------------------------------------                           CASE-switch vs CASE-if
i       | v1   | v2
integer | text | text
      0 | even | foobar
      1 | odd  |
      2 | even |
      3 | odd  | foo
      4 | even |
      5 | odd  | bar
      6 | even | foo
      7 | odd  |
      8 | even |
      9 | odd  | foo
     10 | even | bar
     11 | odd  |
     12 | even | foo
     13 | odd  |
     14 | even |
     15 | odd  | foobar



###
coalesce

В отличие от CASE, функция coalesce просто последовательно вычисляет значения переданных в нее выражений, пока не встретит первый не-NULL'овый результат - его и возвращает:

COALESCE(значение [, ...]) -- функция COALESCE возвращает первый попавшийся аргумент, отличный от NULL. Если же все аргументы равны NULL, результатом тоже будет NULL.

То есть coalesce можно рассматривать как подобный CASE:

CASE
  WHEN valX IS NOT NULL
    THEN valX
  WHEN valY IS NOT NULL
    THEN valY
END
->
coalesce(valX, valY)



Пример:			

SELECT
  i
, coalesce(
    CASE i % 2
      WHEN 0 THEN 'even'
    END
  , CASE
      WHEN i % 15 = 0 THEN 'foobar'
      WHEN i %   3 = 0 THEN 'foo'
      WHEN i %   5 = 0 THEN 'bar'
    END
  )
FROM
  generate_series(0, 15) i;
----------------------------------               Второй CASE выполняется только при ELSE-ветке первого
i       | coalesce    
integer | text  
      0 | even
      1 | 
      2 | even 
      3 | foo
      4 | even 
      5 | bar
      6 | even
      7 | 
      8 | even 
      9 | foo
     10 | even 
     11 |   
     12 | even
     13 | 
     14 | even 
     15 | foobar



Обратной по смыслу к coalesce является функция nullif, принимающая значение NULL при совпадении значений пары аргументов, иначе возвращающая результат вычисления первого:

SELECT
  i
, nullif (
    i % 2
  , i % 3
  )
FROM
  generate_series(0, 15) i;
-----------------------------------
i       | nullif   
integer | integer 
      0 |                -- i % 2 ->  0 == 0  <-  i % 3
      1 | 
      2 |       0        -- i % 2 ->  0 == 2  <-  i % 3
      3 |       1
      4 |       0
      5 |       1
      6 | 
      7 | 
      8 |       0
      9 |       1
     10 |       0
     11 |       1
     12 | 
     13 | 
     14 |       0
     15 |       1




______________________________________________________________________________________
Функции минимума и максимума (least, greatest)

Поскольку min и max уже заняты в SQL в качестве имен агрегатных функций, а потребность вычисления минимального/максимального из списка переданных значений (вовсе не обязательно лишь 2) никуда не делась, для них введены специальные 
функции least и greatest.

Что ж... На этом рассказ о базовых возможностях SQL-запросов в PostgreSQL окончен, и дальше мы поговорим об оптимизации их эффективности. А пока - в бой!







______________________________________________________________________________________
###################################################################
______________________________________________________________________________________
Анализ запросов (ч.1 — как и зачем читать планы)

В этой лекции мы узнаем, что такое план выполнения запроса, как и зачем его читать (и почему это совсем непросто), и о каких проблемах с производительностью базы он может сигнализировать. Разберем, что такое Seq Scan, Bitmap Heap Scan, 
Index Scan и почему Index Only Scan бывает нехорош, чем отличается Materialize от Memoize, а Gather Merge от "просто" Gather.


Как мы говорили на первой лекции, SQL - это декларативный язык. То есть вы сначала должны описать, что вы хотите получить, что ждете от данных, которые должны вам вернуться. А база искренне считает, что она лучше вас знает, как 
конкретно это сделать: какие индексы стоит использовать, как именно, в каком порядке соединять таблицы, как накладывать условия, будут ли они вообще накладываться после соединения или подойдут под WHERE-условие индекса... Здесь 
все - на откуп СУБД. Конечно, вы можете предполагать, что ваш красивый "запрос о 10 джойнах", который вы нарисовали, будет при выполнении выглядеть хорошо... К сожалению, обычно бывает не очень. Обычно это случается из-за того, что 
PostgreSQL всегда уверен в своей правоте, что даже не принимает "подсказки" в отличие от некоторых других СУБД (где вы можете прямо в комментарии в теле запроса написать "хинт", который подскажет базе, как здесь лучше действовать: с 
какой из таблиц начать соединение, каким индексом лучше воспользоваться или, может, не пользоваться ими вообще). В отличие от них, PostgreSQL не хочет работать с подсказками. Точнее, есть небольшой модуль-расширение 
pg_hint_plan (github), который позволяет чуть-чуть "похачить", в ручном режиме подсказав базе, как стоит себя вести относительно каждого конкретного запроса. Но в эксплуатации он достаточно сложен, поскольку вместо прямых указаний 
базе "делай так" превращает "хинты" в рекомендации "не так делай в последнюю очередь", увеличивая "стоимость" операций, о которой мы поговорим далее.

Поэтому, раз PostgreSQL не готов позволить нам настраивать себя "снаружи" в рамках конкретного запроса, и считает, что лучше нас все знает, он готов этим знанием поделиться с нами. То есть с удовольствием готов рассказать нам, как 
конкретно он хочет выполнять наш запрос (или уже выполнил). 


Что такое план и зачем он нужен
То, что база нам готова рассказать - это план выполнения запроса, который представляет из себя некоторую древовидную структуру в виде либо текста, либо (в зависимости от выбранного формата) JSON, XML или YAML. Как правило, вам придется 
сталкиваться именно с текстовым форматом и, много реже, с JSON. Причем его обычно ставят только в системах, предполагающих какую-то автоматизированную обработку логов, поскольку человеку "прочитать" с листа JSON-объект на несколько сотен 
килобайт почти нереально. Впрочем, если вам такой JSON встретился - не отчаивайтесь, наш сервис визуализации планов explain.tensor.ru вполне успешно "переваривает" и их.


План запроса - Дерево в текстовом виде. Каждый элемент - одна из операций: получение данных, построение битовых карт, обработка данных, операция над множеством, соединение, вложенный запрос и т.д. Выполнение плана - обход дерева 
"снизу-вверх". Впрочем, текстовый формат плана хоть и существенно компактнее, прочитать его ничуть не легче, не имея соответствующих навыков - их мы и постараемся наработать в рамках этой лекции. 

1) Во-первых, узлом дерева плана является какая-то атомарная операция, с точки зрения PostgreSQL: каким образом данные надо получить, какой индекс стоит использовать или даже сразу несколько и надо ли строить по ним битовые карты, 
какой вид соединения или операции над множествами надо использовать, ...
2) Во- вторых, когда такое дерево построено на фазе планирования выполнения запроса, оно передается executor'у и представляет для него готовый алгоритм работы, который реализуется при обходе "снизу-вверх". То есть сначала выполняются 
все нижележащие узлы, а только потом их "предок" - и результат самого верхнего "корневого" узла и есть результат выполнения всего нашего запроса.
3) В третьих, на каждом узле плана в ходе реального выполнения запроса аккумулируются фактически затраченные на выполнение поддерева ресурсы: процессорное время, прочитанные блоки данных, задержки доступа к диску, ...

Собственно, ради этих данных о ресурсах мы и хотим получить план - чтобы выяснить "Кто самое слабое звено?" в этом оркестре. План запроса - "Почему тут выполнялось так долго?" - неэффективный алгоритм, неактуальная статистика, нехватка 
ресурсов (CPU, RAM, storage).

Либо мы написали плохой алгоритм - например, заставили PostgreSQL сначала делать соединение, а затем уникализировать "размножившиеся" записи, либо написали запрос "о десяти джойнах", шансы которого эффективно распланироваться крайне 
малы - слишком уж велика становится комбинаторная вероятность, как его можно выполнить "нехорошо".  Если мы подразумеваем, что надо сначала "проджойнить" таблички №1 и №2, а только потом к ним №3, то база об этом может, в целом, не
догадываться. А происходить это может по той простой причине, что у нас неактуальная статистика - ведь алгоритм планирования опирается именно на статистическое распределение данных, поскольку никакой другой компактной информации у него 
просто не существует.  Например, вы берете и "заливаете" в таблицу данные, где идентификаторы 1, 2, 3, 4, 5, ... и так далее до миллиона - база будет искренне считать, что у вас все идентификаторы независимы, различны и у них равномерное
распределение. Все будет работать хорошо, пока вы не "зальете" дополнительно миллион записей с одинаковым идентификатором. После чего, продолжая считать, что распределение все еще нормальное, относительно этого конкретного 
идентификатора она "промахнется", а вы можете получить "боль" при выполнении этого конкретного запроса. Причем она будет выражаться во вполне конкретных вещах. Вы либо потратите избыточное процессорное время, либо "прокачаете" слишком 
много данных - и хорошо, если все они уже окажутся закэшированы в памяти, а то ведь еще придется ожидать чтения с диска. И чтобы это все выяснить, нам как раз и нужен план. Выглядит в логах обычно он примерно как-то вот так или еще 
страшнее.

					Запрос и его план в текстовом формате в логе сервера
Query Text: explain (analyze, buffers, costs off)
SELECT * FROM pg_class WHERE (oid, relname) = (
  SELECT oid, relname FROM pg_class WHERE relkind = 'r' LIMIT 1
);

Index Scan usnig pg_class_relname_nsp_index on pg_class (actual time=0.049..0.050 rows=1 loops=1)
  Index Cond: (relname = $1)
  Filter: (oid = $0)
  Buffers: shared hit=4
  InitPlan 1 (returns $0,$1)
    -> Limit (actual time=0.019..0.020 rows=1 loops=1)
      Buffers: shared hit=1
      -> Seq Scan on pg_class pg_class_1 (actual time=0.015..0.015 rows=1 loops=1)
        Filter: (relkind = 'r'::"char")
        Rows Removed by Filter: 5
        Buffers:shared hit=1



###################
Получаем и читаем план запроса

Чтобы получить от базы план запроса, как и любую другую вещь в PostgreSQL, мы должны выполнить определенную команду - EXPLAIN:

						Синтаксис команды EXPLAIN

EXPLAIN [ ( параметр [, ..] ) ] оператор
EXPLAIN [ ANALYZE ] [ VERBOSE ] оператор

Здесь допускается параметр:
  { ANALYZE | VERBOSE | COSTS | SETTINGS | BUFFERS | WAL | TIMING | SUMMARY } [ boolean ]
  FORMAT { TEXT | XML | JSON | YAML }

Где оператор:
  { SELECT | INSERT | UPDATE | DELETE | CREATE TABLE ... AS ... }


Выполнение просто EXPLAIN SELECT ... даст нам сам план выполнения - то есть покажет, как база собирается выполнять такой запрос, а вот EXPLAIN ANALYZE SELECT ... даст нам план с результатами уже выполненного запроса - 
поэтому EXPLAIN ANALYZE DELETE ... стоит применять с большой осторожностью.

Помимо ANALYZE, команду EXPLAIN можно снабдить дополнительными опциями, которые определяют, что мы увидим (или нет) в плане:
- VERBOSE - возвращаемые узлом столбцы
- COSTS - "стоимость" операции
- SETTINGS - настройки базы, влияющие на выполнение запроса
- WAL - статистика формирования WAL-файлов
- TIMING - время выполнения операции
- SUMMARY - сводные данные планирования/исполнения

Также можно задать один из 4 поддерживаемых форматов.

EXPLAIN ...
  как СУБД планирует выполнять запрос
  не изменяется от запуска к запуску
  на основе накопленной статистики о данных базы

Получаемый с помощью EXPLAIN план зависит только от накопленной базой статистики распределения данных и не меняется от запуска к запуску, если статистика не изменялась сильно или мы какие-то настройки сервера не "подкрутили".



###################
Структура узлов плана

Если мы выполним EXPLAIN самого простого запроса SELECT * FROM, возвращающего все записи из системной таблички pg_class, которая есть в любой базе, то увидим одну строку:

EXPLAIN SELECT * FROM pg_class;
-------------------------------------------------------Атрибуты строки плана
Seq Scan on pg_class  (cost=0.00..34.90 rows=418 wigth=265)
--------    --------   ----------------  ------- ----------
      |         |              |             |        |
      |  	|              |             |   "ширина" строки (средний размер в байтах)
      |         |              |    плановое кол-во строк (на основе статистики)
      |         |   "стоимость" операции (вместе с дочерними узлами)
      |    имя объекта базы (таблицы, индексы, функции, ...)
тип узла (что делать с данными)


Эта строка содержит в себе некоторые данные о конкретном узле плана. До скобок - это тип узла (что мы делаем с данными) и имя объекта базы (таблицы, индекса, функции, ...), с которым это происходит. В нашем примере мы последовательно 
сканируем (Seq Scan) таблицу pg_class. А внутри первых скобок (потому что потом возникнут еще и вторые) записано то, что база об этой операции "думает": ее условная "стоимость" в "попугаях" - 34.90, вернет она нам 418 строк и их 
"ширина" в среднем составит 265 байт. Тут "стоимость" - это некоторая абстрактная расчетная величина, вычисляемая от заданных в БД параметров и планового количества записей. Такими параметрами выступают "стоимость" обращения к таблице, 
"стоимость" извлечения из нее одной записи, "стоимость" одной атомарной операции обработки над ней с точки зрения CPU, ...   Фактически, "стоимость" является тем самым камнем на распутье ("Налево пойдешь - коня потеряешь, 
направо - голову сложишь."), который позволяет PostgreSQL принять решение, какой из возможных планов (а их вариантов могут получаться сотни и тысячи) лучше/хуже. То есть, с точки зрения СУБД, нет "хороших" планов - есть только плохие. 
И из всего множества планов, которые она способна "придумать", старается выбрать наименее "плохой" - то есть имеющий минимальную "стоимость".



###################
Управление планами и "тюнинг" сервера

Например, мы можем захотеть не просто прочитать все записи из pg_class, но и отсортировать по столбцу relname - для этого как раз есть подходящий индекс:

Случай 1:
EXPLAIN SELECT * FROM pg_class ORDER BY relname;
--------------------------------------------------------------------
Index Scan using pg_class_relname_nsp_index on pg_class   (cost=0.27...44.15) rows=418 width=265)


Случай 2:
SET enable_indexscan = off;
EXPLAIN SELECT * FROM pg_class ORDER BY relname;
------------------------------------------------------------------                                      Seq Scan "дешевле", но необходимость сортировки всё усложняет
Sort   (cost=53.10...54.14 rows=418 width=265)
  Sort Key: relname
  ->  Seq Scan on pg_class    (cost=0.00..34.90 rows=418 width=265)


Использование индекса в плане мы видим как узел Index Scan , "стоимость" которого 44.15 хоть и выше, чем 34.90 для Seq Scan, зато позволяет избежать лишней операции сортировки и дает более "дешевый" суммарно план. Кстати, в этом 
примере мы "выключили" возможность использовать Index Scan с помощью команды SET и параметра enable_indexscan. Таких "настроечных" параметров несколько десятков(https://postgrespro.ru/docs/postgresql/16/runtime-config-query), но 
использовать их надо с большой осторожностью:

"
Эти параметры конфигурации дают возможность ГРУБО влиять на планы, выбираемые оптимизатором запросов. Если автоматически выбранный оптимизатором план конкретного запроса оказался неоптимальным, в качестве ВРЕМЕННОГО решения можно
воспользоваться одним из этих параметров и вынудить планировщик выбрать другой план.

SET enable_{operation} = off [on];

Улучишить качество планов, выбираемых планировщиком, можно и более подходящими способами, в частности, скорректировать константы стоимости ...

SET {operation}_cost = {floating point};
*_page_cost, *_tuple_cost, *_setup_cost, *_operator_cost, jit_*_above_cost, *_size
"


С их помощью мы можем запретить серверу использовать ту или иную операцию в планах, но не можем заставить его применять ровно ту, которую хочется нам. Зато, управляя "стоимостью" отдельных операций типа обращения к странице данных или 
обработки отдельной строки, можем попытаться побудить его делать это. Например, при использовании в качестве хранилища какого-нибудь ленточного накопителя мы можем задать, что seq_page_cost ("стоимость" извлечения страницы данных при 
последовательном чтении) будет совсем небольшим, а вот random_page_cost ("стоимость" обращения к произвольной странице) - запредельным. 

Если мы хотим увидеть в плане значения оказавших влияние параметров, используем опцию SETTINGS, которая добавит строку Settings: ... с набором установленных параметров и их значений в конец плана:

EXPLAIN (SETTINGS) SELECT * FROM pg_class;
---------------------------------------------------------	Параметры сервера в выводе плана
Seq Scan on pg_catalog.pg_class  (cost=0.00..34.90 rows=418 width=265)
Settings: constraint_exclusion = 'on', cpu_tuple_cost = '0.05', effective_cache_size = '44GB',
  effective_io_concurrency = '128', random_page_cost = '1.1', temp_buffers = '256MB',
  work_mem = '512MB'



###################
Объем данных

Следующая пара значений, которые нам выводит EXPLAIN - это "длина" и "ширина" данных. То есть предполагаемое количество записей, которое должен вернуть этот узел "вверх" по дереву плана и средняя "ширина" строки в байтах, определяемая на 
основании размерности типов тех данных, которые мы хотим получить в каждом из полей (Ориентировочный объём необходимой памяти, т.е. можно оценить объём resultset и памяти).

Понять, что это за поля, нам поможет опция VERBOSE, которая к каждому узлу добавит атрибутную строку Output: ... со списком столбцов:

			Количество и "ширина" строк (VREBOSE позволяет увидеть возвращаемые поля)
Случай 1:
EXPLAIN VERBOSE SELECT * FROM pg_class;
------------------------------------------------------
Seq Scan on pg_catalog.pg_class    (cost=0.00..34.90 rows=418 width=265)   
  Output: oid, relname, relnamespace, reltype
					418 x 265 = 110770 = 108.1KB

Случай 2:
EXPLAIN VERBOSE SELECT oid FROM pg_class;
Seq Scan on pg_catalog.pg_class    (cost=0.00..34.90 rows=418 width=4)
  Output: oid
					418 x 4 = 1672 = 1.6KB

Output - поля, возвращаемые узлом


Банально умножив rows x width, можно грубо оценить, что SELECT oid вернет в нашем примере примерно 1.6KB данных, а SELECT * - в 66 раз больший объем! Поэтому, если вдруг вы где-то по привычке и собственного удобства для пишете 
SELECT * из достаточно "широкой" таблички, а потом из всех полученных полей используете лишь одно-два, задумайтесь, насколько ваше удобство весомо относительно нагрузки на СУБД - база ведь достаточно "наивная" и делает ровно то, что 
вы ей написали.



###################
Фактические показатели выполнения плана

Все описанные выше метрики можно получить без реального выполнения запроса. Но если надо увидеть именно реальные показатели, протокол фактического выполнения, необходимо воспользоваться формой EXPLAIN ANALYZE ... или "скобочной" 
опцией ANALYZE:


				Синтаксис EXPLAIN ANALYZE
EXPLAIN ANALYZE ...
EXPLAIN (ANALYZE [, ...]) ...
  протокол фактического выполнения запроса
  EXPLAIN ANALYZE DELETE - не лучшая идея
 

  BEGIN;			-- завернём выполнение в транзакцию
  EXPLAIN ANALYZE ...;
  ROLLBACK;			-- откатим изменения транзакции


Если EXPLAIN говорит нам "планирую делать так", то EXPLAIN ANALYZE - "планировал делать вот так и в ходе выполнения получил такие-то показатели".  Причем надо понимать, что реальное выполнение запроса в этот момент так и происходит. 
То есть делать EXPLAIN ANALYZE DELETE, не завернув в откатываемую транзакцию, точно не стоит - можно лишиться части нужных данных. Да и EXPLAIN ANALYZE SELECT бесконтрольно, особенно на бою, тоже запускать не стоит, если там вызывается 
какая-нибудь функция с сайд-эффектами - будет беда. Не факт, что будет обязательно, но запросто может случиться... 

Но если мы все сделали правильно, то увидим у каждого узла еще и вторые скобочки, а в них - среднее время однократного выполнения в миллисекундах (actual time), фактическое среднее количество возвращенных строк за один цикл(rows) и 
количество циклов (loops) выполнения этого узла:

					Фактические показатели выполнения

EXPLAIN ANALYZE SELECT * FROM pg_class;
Seq Scan on pg_class   (cost=0.00..34.90 rows=418 width=265) (actual time=0.004..0.044 rows=418 loops=1)
Planning Time: 0.056 ms      <=======\\                                          ------    ----      ---
Execution Time: 0.085 ms     <===\\  ||                                            |         |        |                
                                 ||  ||          время получения первой/последней строки     |        |
            общее время выполнения   ||                                                      |        |
                                     ||                                  фактическое кол-во строк     |
                                  время планирования                                                  |
                                                                                                кол-во повторов

Plainning/Execution Time - время для всего плана


Например, для плана "для каждой записи в таблице A найди подходящую в таблице B" у узла чтения из таблицы A будет всего один цикл, который вернет все N записей (rows=N loops=1), а вот у узла чтения из таблицы B будет N циклов по одной 
записи (rows=1 loops=N). Дополнительно, мы обычно получаем строки Planning Time, отражающую время, тоже в миллисекундах, затраченное на планирование запроса (да-да, оно ни разу не бесплатное!), и Execution Time, содержащую полное время 
самого выполнения, включая передачу результата клиенту. То есть в нашем примере на планирование ушло 56 микросекунд, а на все выполнение - еще 85. Причем, 44 из них - по плану, а еще 41 "потерялось" на передаче данных.

Но если эта информация нам неинтересна, мы можем "выключить" ее с помощью опции SUMMARY off, равно как и "стоимости" через COSTS off:

  						"Выключаем" часть избыточной информации
EXPLAIN (ANALYZE, COSTS off) SELECT * FROM pg_class;
Seq Scan on pg_class   (actual time=0.004..0.044 rows=418 loops=1)
Planning Time: 0.056 ms
Execution Time: 0.085 ms

EXPLAIN (ANALYZE, SUMMARY off) SELECT * FROM pg_class;
Seq Scan on pg_class   (cost=0.00..34.90 rows=418 width=265) (actual time=0.004..0.044 rows=418 loops=1)

EXPLAIN (ANALYZE, COSTS off, SUMMARY off) SELECT * FROM pg_class;
Seq Scan on pg_class   (actual time=0.004..0.044 rows=418 loops=1)


Если мы "выключили" COSTS off, то скобочки у узла останутся только одни, но показатели в них будут - реальные, actual time. А вот если в этих скобках будет написано (never executed), то узел вовсе не выполнялся - как в случае 
отсутствия записей в таблице A из примера выше:

					Часть узлов в плане может так и не начать выполняться
EXPLAIN (ANALYZE, COSTS off, SUMMARY off)
  SELECT * FROM pg_class LIMIT 0;
---------------------------------------------------------
Limit (actual time=0.001..0.001 rows0 loops=1)
  -> Seq Scan on pg_class (never executed)
                           -----------------
                                    |
                            узел не выполнялся




###################
На что обратить внимание

1) Если время планирования запроса сравнимо со временем его выполнения или, тем более, больше, то стоит крепко задуматься, стоит ли это планирование выполнять каждый раз. Ведь если мы один раз поняли, что нам из этой таблички 
эффективнее всего читать Seq Scan'ом, она невелика, и других вариантов особо не предвидится, то зачем тратить на это время каждый раз для каждого запроса?

			Время планирования

Seq Scan on pg_class   (cost=0.00..34.90 rows=418 width=265)
                                        (actual time=0.004..0.044 rows=418 loops=1)
Planning Time: 0.056 ms    <-------\\                       ------
Execution Time: 0.085 ms           ||                         ||
		                   ||                         ||
	                время планирования сравнимо со временем исполнения - путь к prepared statements (подготовленные запросы): PREPARE, EXECUTE, DEALLOCATE


Этого можно избежать - для этого существует функционал подготовленных запросов (prepared statements) (https://postgrespro.ru/docs/postgresql/16/sql-prepare). В документации вы можете познакомиться с командами PREPARE, EXECUTE 
и DEALLOCATE, которые позволяют подготовить план запроса однократно. Он "сохранится" на вашем подключении к базе, а дальше вы будете по его имени только подставлять в него необходимые значения ("дай пользователя с логином Petya", 
"дай пользователя с логином Vasya", ...) - заодно и трафик до сервера СУБД сэкономите, раз текст самого запроса каждый раз передавать уже не надо. 


2) Второй момент, который в плане достаточно хорошо заметен - это несовпадение планируемого и фактического количества строк, возвращаемых узлом.

     
				               //====================\\
                                             ----                    ||
Seq Scan on pg_class   (cost=0.00..34.90 rows=418 width=265)         | \    сильная разница в любую сторону - неактуальная статистика
                       (actual time=0.004..0.044 rows=418 loops=1)   | /
Planning Time: 0.056 ms                               ----           ||
Execution Time: 0.085 ms                               \\============//


Сильное отклонение в любую сторону - это плохо, оно дезинформирует базу, из-за чего она может выбирать неэффективные планы - и с этим "плохо" надо бороться. Как правило, это вызвано неактуальностью статистики после большого по объему 
удаления данных, или импорта данных с другим распределением - как я рассказывал в примере в начале. Если база самостоятельно автоматически не успела пересобрать статистику, она будет ошибаться при планировании ваших запросов. Выправить 
ситуацию нам помогут команды ANALYZE (https://postgrespro.ru/docs/postgresql/16/sql-analyze) и VACUUM ANALYZE (https://postgrespro.ru/docs/postgresql/16/sql-vacuum) - первая из них позволяет "пересобрать" статистику по таблице, а 
вторая еще и разметит пространство под удаленными/измененными версиями записей как свободное.

Но все-таки лучше заставить PostgreSQL делать все это самостоятельно, правильно настроив параметры autovacuum (https://postgrespro.ru/docs/postgresql/16/runtime-config-autovacuum) для каждой из таблиц в соответствии с их смыслом:


				Актуальность статистики (Параметры autovacuum можно задать для таблицы)
	ALTER TABLE ... SET (...)
	
	autovacuum_analyze_threshold - задаёт минимальное число добавленных, изменённых или удалённых кортежей, при котором будет выполняться ANALYZE для отдельно взятой таблицы. Значение по умолчанию - 50 кортежей.
	
	autovacuum_analyze_scale_factor - задаёт процент от размера таблицы, который будет добавляться к autovacuum_analyze_threshold при выборе порога срабатывания команды ANALYZE. Значение по умолчанию = 0.1 (10% от размера таблицы).
	



Но если у нас все настроено, статистика актуализируется вовремя, но что-то нам все равно не подходит, это может быть вызвано тем фактом, что статистика у нас все-таки недостаточна. Например, в таблице у вас есть миллион разных 
идентификаторов с разной частотой появления для разных их диапазонов, а база по умолчанию собирает данные о распределении относительно всего лишь 100 значений (параметр default_statistics_target). Понятно, что эти значения будут 
достаточно равномерно распределены по всему миллиону, но между ними будут интервалы по 10K значений, где распределение может быть совсем-совсем другим. Поэтому иногда для столбцов, содержащих большое количество уникальных значений, 
имеет смысл увеличить количество отсчетов гистограммы распределения с помощью команды SET STATISTICS (https://postgrespro.ru/docs/postgresql/16/sql-altertable#SQL-ALTERTABLE-DESC-SET-STATISTICS), например, поставив 
вместо 100 значение 10000:


				Актуальность статистики (Улучшаем доступную статистику)
	Недостаточная статистика
	ALTER TABLE ... ALTER COLUMN ...
	  SET STATISTICS [0..10000]
	CREATE STATISTICS


Другой вариант - создать отдельный срез расширенной статистики (https://postgrespro.ru/docs/postgresql/16/planner-stats#PLANNER-STATS-EXTENDED) сразу по нескольким столбцам одной таблицы с помощью команды CREATE STATISTICS - подойдет для 
функционально связанных данных. Это может быть полезно, например, когда у вас типовая задача искать сразу по паре полей в EAV-представлениях объектов, или идентификатор пользователя является хэшем от его логина и поля имеют зависимые 
распределения.




###################
Делаем выводы

Но вернемся к самим узлам плана и посмотрим, почему умение читать и понимать план - это искусство, как сказано в документации (https://postgrespro.ru/docs/postgresql/16/using-explain).  

1) Начнем с самого простого, что из плана можно "вычитать":

				Время выполнения (Время узла надо умножать на количество циклов)

Seq Scan on pg_class   (cost=0.00..34.90 rows=418 width=265)
                                        (actual time=0.004..0.044 rows=418 loops=1)          --          loops * actual time = 1 *0.044  -> полное время выполнения
Planning Time: 0.056 ms                                     ------               --
Execution Time: 0.085 ms 



В общем-то про время сказать особо нечего, кроме того, что это walltime - то есть при оптимизации параллельных запросов (https://habr.com/ru/post/514112/) гораздо эффективнее уделять внимание полному времени выполнения, суммарному по 
всем worker-процессам.


2) Следующий немаловажный параметр - объем передаваемых данных - "вверх" по дереву между узлами или "наружу", если это корневой узел:

				Сетевой трафик ("Ширину" записи - на их количество и снова на количество циклов)

Seq Scan on pg_class   (cost=0.00..34.90 rows=418 width=265)
                                                        -----
                                        (actual time=0.004..0.044 rows=418 loops=1)          -- loops * actual rows * width = 1 * 418 * 265 -> размер resultset
Planning Time: 0.056 ms                                                ----      --
Execution Time: 0.085 ms 


В нашем примере мы вычитываем из базы 418 строк "шириной" 265 байт, что дает примерно 108KB трафика. А теперь просто представьте, что это делают одновременно по какому-нибудь сигналу 100 подключенных клиентов - и вот мы уже получили в 
пике почти 100 мегабит трафика с сервера БД! А дальше варианты, каких ресурсов не хватит раньше: производительности CPU сервера, объема сетевого буфера или пропускной способности сети...

И тут надо понимать, что если "внутри" PostgreSQL достаточно хорошо оптимизирован, и все повторные результаты может почти мгновенно формировать на основе кэша, то "наружу" он отдаст вам ровно то, что вы просите. Если на уровне драйвера 
работы с ним вы используете получение результата в подобном кейсе по одной записи чем-то вроде .fetchOne, то это еще ничего, а вот использование .fetchAll или "свертка" всего результата в JSON заставляет сервер одномоментно "выплюнуть" 
во внешний мир тонну трафика - лучше не стоит до такого доводить. 


3) От "внешних" сетевых вещей погрузимся внутрь сервера: опция BUFFERS позволяет увидеть количество 8KB-страниц данных (подробнее, как устроено хранение данных в PostgreSQL, можно прочитать в документации), которые пришлось откуда-то 
прочитать (из памяти или с носителя): 

	Buffers      Объём прочитанных/записанных данных (страниц) (А вот объем прочитанных данных умножать ни на что не надо!)
	
	EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM pg_class; 
	----------------------------------------------------------------------------
	Seq Scan on pg_class (cost=0.00..34.90 rows=418 width=265) (actual time=0.017..0.109 rows=418 loops=1)
 	  Buffers: shared hit=14					-- 14
 	Planning:
	  Buffers: shared hit=107 read=12 written=11			-- 107, 12, 11
	  I/O Timings: shared/local read=18.118 write=12.037
	Planning Time: 30.910 ms
	Execution Time: 1.162 ms
	-----------------------------------------------------------------------------
	N (pages) * 8KB(размер страницы) = физический объём
		SHOW block_size;


В отличие от времени выполнения, количества строк или объема трафика, Buffers- значения уже не надо умножать на количество циклов узла - они и так просуммированы. Зато если умножить на "умолчательные" 8KB, можно вычислить 
физический объем. При этом мы сразу видим, откуда данные были прочитаны, и можем оценить, насколько быстро/медленно это происходило:

                Buffers	(Варианты источников данных)


-------------------|------------|------------|-------------|------------
		   | hit	| read 	     | dirtied     | written
		   | из памяти  | с диска    | в памяти    | на диск
		   | получено   | прочитано  | "испачкано" | записано
-------------------|------------|------------|-------------|------------
       local	   | очень      | медленно   | быстро      | медленно
 локальная память  | быстро     |            |             |
 процесса          |            |            |             |
-------------------|------------|------------|-------------|------------
       shared	   | быстро	| медленно   | быстро      | медленно
 разделяемая	   |            |            |             |
 память сервера	   |            |            |             |
-------------------|------------|------------|-------------|------------
       temp        |    ---     | медленно   |   ---       | медленно    
  временные файлы  |            |            |             | 
-------------------|------------|------------|-------------|------------

Buffers: shared hit=12673094, local hit=7 read=1 dirtied=1, temp read=11402 written=11402

- local - самая быстрая работа с локальной памятью процесса, обслуживающего наше соединение с базой
- shared - все еще быстрое обращение к разделяемой между всеми процессами памяти сервера, где уже могут включаться семафоры для разделения доступа
- temp - работа с временными файлами
- hit - получение из памяти
- read - чтение с диска с помещением в память
- dirtied - количество страниц, которые кто-то успел перезаписать, пока наш запрос до них добирался
- written - количество записанных нами страниц

Наиболее часто встречающиеся на практике комбинации:

- shared hit - обычное быстрое чтение из кэш-памяти сервера
- shared read - медленное и грустное чтение с диска
- local written/hit - обращение к небольшой временной таблице (TEMPORARY TABLE)
- temp written - сброс большого объема данных во временный файл - например, для сортировки миллиарда записей в ходе выполнения запроса


Но тут надо отметить, что buffers - это не "сами" прочитанные данные, а их "трафик". То есть если нам пришлось 100 раз обратиться к одной и той же странице данных (например, вся таблица из одной страницы и состоит), то мы увидим в плане
именно значение 100, а не 1.

Тут важно отметить, что время выполнения может достаточно сильно меняться между запусками одного и того же (особенно "быстрого") запроса из-за нагрузки на сервер СУБД, наведенной процессами-соседями. А вот суммарные значения buffers 
остаются практически неизменны и представляют хороший критерий для оценки оптимизации запроса. Разве что отсутствующие в кэше или ранее вытесненные из него страницы при первом запуске окажутся в shared read, а при последующих 
"переедут" в shared hit.

3.1 ) Одна из типовых проблем, которая хорошо заметна при оптимизации "по buffers" - большой объем shared read - читаемого с диска. Традиционными симптомами являются периодические крики "Да вы одни всю СХД загрузили!" от 
админов-"железячников"и неудовлетворительное время отработки запросов (СХД - система хранения данных).

        Как уменьшить shared read

	shared read - чтение с диска при отсутствии в кэше
	не надо читать так много!
	увеличить shared_buffers
	увеличить память на сервере



Для решения подобной проблемы есть два пути - быстрый и правильный. Раз у нас данные читаются с диска, а не из памяти (что, конечно, было бы много быстрее) - значит, этой самой памяти нам не хватает! (Ваш К.О.) Поэтому для 
быстрого "затыкания дыры" подойдет возможность быстро "докинуть" памяти - это может быть как физическое увеличение на сервере, если база у вас постепенно со временем "подросла", и теперь весь доступный объем уже используется 
под pagecache, так и банальное увеличение параметра shared_buffers (https://postgrespro.ru/docs/postgresql/16/runtime-config-resource#GUC-SHARED-BUFFERS), если сервер был сконфигурирован несоответствующим нагрузке образом.

Конечно, увеличение shared_buffers может быть актуально, только если разные подключения к базе читают одни и те же данные, как правило. Если же, например, каждый коннект пишет-читает только "свои" сегменты данных, и они никак не 
пересекаются, то эффекта не будет. А вот правильный путь гораздо сложнее - не читать так много, особенно лишнего. Например, банальное отсутствие подходящего индекса (https://habr.com/ru/post/659889/) заставит PostgreSQL прочитать много 
больше данных, чем стоило бы, равно как и неправильно сделанное секционирование (https://habr.com/ru/post/483170/) или плохо реализованная постраничная навигация (https://habr.com/ru/post/498740/). То есть, как правило, правильный путь 
для уменьшения чтений с диска - изменение самого алгоритма работы запроса, что может быть недешево.


3.2) На другой чаше весов - запись на диск. И если избежать записи самих данных не удастся никак, то вот над объемом временно записываемых файлов temp written можно "поколдовать".

			Как уменьшить temp written

	temp written - сброс на диск промежуточных выборок
	не надо обрабатывать так много!
	увеличить work_mem
	вынести temp_tablespaces  на RAMdrive


И базовой рекомендацией опять-таки будет обратить внимание на ваш алгоритм. Вспомните слайд, где мы отключали возможность использования индекса и получили Seq Scan + Sort - понятно, что если нам придется для сортировки все вычитанные 
записи куда-то на диск предварительно "свапнуть", это не будет работать быстро. А с подходящим индексом, который вообще избавит нас от лишней операции сортировки - будет! Кстати, от сортировок можно избавляться 
(https://habr.com/ru/post/522114/) разными способами, а не только созданием индексов. Но если вдруг сортировать все равно надо, то стоит посмотреть, сколько там надо места-то. Возможно, стоит просто немного увеличить значение параметра 
work_mem (https://postgrespro.ru/docs/postgresql/16/runtime-config-resource#GUC-WORK-MEM) с исходных 4MB пропорционально возможностям сервера и сложности ваших запросов (поскольку этот объем выделяется под узел плана). В этом случае 
операция будет происходить в памяти, без "свапа" на диск, с гораздо большей вероятностью, а ваш диск вздохнет с облегчением.

Но когда данных так много, что ни в какую память узла они не помещаются, можно рассмотреть вариант с промежуточной "ручной" выгрузкой в TEMPORARY TABLE, а их - вынести на RAMdrive с помощью 
параметра temp_tablespaces (https://postgrespro.ru/docs/postgresql/16/runtime-config-client#GUC-TEMP-TABLESPACES).


3.3) Следующая атрибутная строка I/O Timings, содержащая время, затраченное на обращение к диску в миллисекундах, может появиться в узлах плана, если на вашем сервере включен 
параметр track_io_timing (https://postgrespro.ru/docs/postgresql/16/runtime-config-statistics#GUC-TRACK-IO-TIMING) (или вы включили его принудительно с помощью команды SET):


					I/O Timigns (Время доступа к диску умножать - тоже не надо)
	Время чтения/записи на диск (ms)
	SET track_io_timing = on; EXPLAIN (ANALYZE, BUFFERS) SELECT ...
	--------------------------------------------------------------------------------
	Seq Scan on pg_class   (cost=0.00..34.90 rows=418 width=265) (actual time=0.017..0.109 rows=418 loops=1)         
	  Buffers: shared hit=14
	Planning:
	  Buffers: shared hit=107 read=12 written=11
	  I/O Timings: shared/local read=18.118 write=12.037
	  --------------                 -------      -------

	Planning Time: 30.910 ms                                
	Execution Time: 1.162 ms 


read/write-время ходит обычно вместе с read/write-buffers, и, поделив одно на другое, мы можем узнать "скорострельность" нашего диска с базой в моменте выполнения запроса:

					I/O Timings (Определяем скорость диска)
	Buffers: shared hit=107 read=12 written=11
                                     ---        ---
	I/O Timings: shared/local read=18.118 writte=12.037
 	                               --------      --------
	Read = 12 * 8KB / 18.118ms = 5.17MB/s
	Write = 11 * 8KB / 12.037ms = 7.14MB/s


Если она оказывается сильно меньше, чем вы ожидали, то или всю его пропускную способность "пожрал" какой-то соседний процесс - вполне возможно вовсе не имеющий отношения к PostgreSQL, или он уже сильно деградировал на аппаратном уровне,
и пора его заменить - например, SSD, который выдает единицы MB/s вместо GB/s. Правда, тут необходимо разделять последовательный доступ к данным в Seq Scan-узлах, который на хороших SSD может и несколько GB/s демонстрировать и 
произвольный во всяких Index Scan, который всегда будет заведомо медленнее.

Подводя итоги анализа показателей узлов плана, вспомним еще раз фразу про "понимать план - это искусство". Поскольку для решения нашей исходной задачи "Кто самое слабое звено?" нам необходимо взять с узла его показатели, часть из 
них (actual time, rows) умножить на loops, а часть (Buffers, I/O Timings) - нет, а затем из полученного вычесть данные с подузлов по иерархии дерева:

					Собственные показатели узла (Что-то надо умножать, а что - нет)
	EXPLAIN (ANALYZE, BUFFERS, COSTS off, SUMMARY off)
	  SELCET * FROM pg_class LIMIT 1;
	------------------------------------------------------------------
	Limit (actual time=0.017..0.018 rows=1 loops=1)                      -- 0.018 x 1 - 0.016 x 1 =  0.002
	  	                  ------            ---                      --         1 -         1 =      0
	  Buffers: shared hit=1
	                     ---
	  -> Seq Scan on pg_class (actual time=0.015..0.016 rows=1 loops=1)
	                                              ------             ---
	    Buffers: shared hit=1
	                                     ---
	
	loops * actual time - subnodes -> собственное время
	Buffers - subnodes -> собственные данные
	I/O Timings - subnodes -> собсвтенные I/O-задержки


При этом нельзя забывать о проблемах (https://habr.com/ru/post/477624/) округления времени (оно выводится с точностью лишь до микросекунд, что при больших значениях loops дает ощутимую погрешность) и распределения ресурсов между CTE...
В общем, пользуйтесь нашим сервисом визуализации планов explain.tensor.ru, и не будете знать горя! Если вдруг не доверяете безопасность своих данных сторонним сервисам - можете приобрести self-hosted версию для развертывания в 
собственном закрытом контуре. Если вам надо анализировать производительность сервера PostgreSQL в целом - включите на нем модуль auto_explain, и наш сервис поможет и здесь (https://habr.com/ru/post/696804/), предоставляя возможность 
сквозного анализа проблем от распределения нагрузки между типами запросов до интеллектуальных подсказок к конкретному неудачному плану.

В следующей части поговорим, а что это вообще за типы узлов - Seq Scan, Index Scan, какие они вообще бывают и чем отличаются друг от друга.




______________________________________________________________________________________
###################################################################
______________________________________________________________________________________
Анализ запросов (ч.2 - узлы получения данных)

В первой части лекции мы узнали, что такое план выполнения запроса, как и зачем его читать (и почему это совсем непросто), и о каких проблемах с производительностью базы он может сигнализировать. В этой - разберем, что такое Seq Scan, 
Bitmap Heap Scan, Index Scan и почему Index Only Scan бывает нехорош.

Наконец, мы добрались до "основного блюда" нашей лекции - перечня тех операций, про которые план как раз может нам рассказать:

			        Операции в плане (Типы узлов в плане)

	Получение данных				Группировка
	Result, ... Scan				Aggregate, Group, GroupAggregate, ...
	
	Построение битовых карт				Соединения
	Bitmap... 					Nested Loop, Hash/Merge Join, ... Semi/Anti Join
	
	Вложенные запросы				Параллелизм операций
	InitPlan, SubPlan				Parallel ..., Gather, Gather Merge
	
	Операции над множествами			Изменение данных
	Append, Intersect, Except, SetIp, ...		Insert, Update, Delete, merge

	Обработка					Триггеры/Foreign Keys
	Limit, Sort, Incremental Sort, Unique		Trigger, Trigger for constraint



И самые важные из них - *операции получения данных* - основной функционал, ради которого мы с СУБД и работаем. Поэтому их видов больше всего - наверное, даже больше, чем всех остальных вместе взятых.



______________________________________________________________________________________
"Нематериальные" источники данных

Result

Первое, с чего мы начнём - узел Result, который вы можете увидить в плане самого перостого запроса:

	Result (константа)

	Константный результат
	EXPLAIN SELECT 1;
	-----------------------
	Result (cost=0.00..0.05 rows=1 width=4)


Он появляется, когда вы просто хотите получить в столбце результирующей выборки какое-то константное значение (например, SELECT 1) или результат вычисления простой функции:

	Result (функция в поле)

	Вызов простой функции в поле
	EXPLAIN SELECT random();
	------------------------------------------------
	Result (cost=0.00..0.05 rows=1 width=8)


"Простой" не в смысле сложности производимых ей операций, конечно, а возвращающей некоторое единичное значение, а не целую SETOF-выборку (https://postgrespro.ru/docs/postgresql/16/xfunc-sql#XFUNC-SQL-FUNCTIONS-RETURNING-SET).

Третий вариант, когда может возникнуть Result-узел - "однострочная" команда VALUES, о которой мы говорили на второй лекции:

	Result (однострочный VALUES)
	
	Чтение из однострочного VALUES
	EXPLAIN VALUES (1);
	-----------------------------------------
	Result (cost=0.00..0.05 rows=1 width=4)




###################
Values Scan

Тут как раз важно, чтобы строка была единственная. Потому что когда присутствует несколько строк внутри VALUES, уже возникает узел Values Scan, в rows-атрибуте у которого будет как раз количество этих самых строк:

	Values Scan (Многострочный VALUES)

	Чтение из многострочного VALUES
	EXPLAIN VALUES (1), (2);
	-------------------------------------------
	Values Scan on "*VALUES*"     (cost=0.00..0.11 rows=2 width=2)
		       ------------
		           |
		имя источника данных (таблицы, функции, выборки)


В качестве источника данных будет указано имя "*VALUES*" или какой-то из его "номерных" вариантов типа "*VALUES*_1", отличать которые друг от друга можно как раз по количеству возвращаемых ими записей - потому что на момент планирования 
выполнения запроса база прекрасно знает, сколько конкретно их там будет, поскольку вы их все в тело SQL-запроса переписали, и больше им взяться неоткуда.



###################
Function Scan

Выше мы видели результат обращения к простой функции в поле, а если указать генерирующую выборку функцию и во FROM-часть запроса, как, например, generate_series, которую рассматривали в лекции про "сложные SELECT" ?.. Тогда мы увидим в 
плане узел Function Scan:

	Function Scan (Выборка из функции)

	Генерирующая функция во FROM
	EXPLAIN SELECT * FROM generate_series(1, 4);
	----------------------------------------------------------
	Function Scan on generate_series (cost=0.00..0.20 rows=4 width=4)


	EXPLAIN SELECT * FROM generate_series(1, 4) i;				-- i алиес
	----------------------------------------------------------
	Function Scan on generate_series i (cost=0.00..0.20 rows=4 width=4)	-- i алиес



Тут, дополнительно к имени самой вызываемой функции, из запроса в узел будет странслирован и указанный для нее алиас, по которым можно сопоставлять узлы с разными точками вызова одних и тех же функций в теле SQL.



###################
Атрибуты фильтрации (Filter / Rows Removed by Filter)

Раз мы дошли до узлов, которые что-то берут из базы (конкретно тут мы ничего еще пока не взяли из таблиц, но уже взяли из функции), то мы можем в запросе наложить некоторое условие. Раз у нас есть FROM-часть в запросе, то может 
присутствовать и WHERE-условие:

	Filter (Условие отбора записей)

	Условие фильтрации записей
	EXPLAIN SELECT * FROM generate_series(1, 4) i WHERE i = 1;	<------------------\
	-------------------------------------------------------------------------------	    \ условие
	Function Scan on generate_series i (cost=0.00..0.21 rows=1 width=4)                 /
	  Filter: (i=1)     <--------------------------------------------------------------/



В этом случае у узла мы получим атрибутную строку Filter, в которой и будет записано наше условие отбора записей. Точнее, почти оно - в том виде, в который его преобразует PostgreSQL для конкретного узла: например, n IN (2, 3, 5) будет 
преобразовано в n = ANY('{2,3,5}'::integer[]), а сложное условие по нескольким функциям или таблицам может "расползтись" частями на несколько узлов плана. А раз есть условие отбора, то при фактическом исполнении запроса (или при вызове 
EXPLAIN ANALYZE) могут возникнуть и строки, которые этот отбор не прошли - их количество выводится в атрибутной строке Rows Removed by Filter:

	Rows Removed by Filter

	Количество записей, отброшенных по условию
	EXPLAIN (ANALYZE, COSTS off, SUMMARY off)
	SELECT * FROM generate_series(1, 4) i WHERE i =1;
	--------------------------------------------------------------
	Function Scan on generate_series i (actual time=0.009..0.010 rows=1 loops=1)
	  Filter: (i=1)
	  Rows Removed by Filter: 3


В нашем примере, очевидно, условию будет соответствовать только лишь первая строка. Но база об этом ничего не знает, поэтому честно генерирует все 4 строки, 3 отбрасывает (Rows Removed by Filter: 3), а 1 - возвращает нам (rows=1). 
Фактически, уменьшение значения RRbF - непроизводительной работы базы, когда записи сначала генерируются/вычитываются, а потом отбрасываются - один из способов оптимизации эффективности выполнения SQL-запросов. В труднодостижимом 
идеальном состоянии, этой атрибутной строки у вас вообще не должно быть.



###################
ProjectSet

Но если мы обратимся к генерирующей функции в поле, то вместо Function Scan получим совсем другой узел - ProjectSet:

	ProjectSet (SETOF-функция в поле)

	Генерирующая функция в поле
	EXPLAIN SELECT generate_series(1, 4);	-- Set-Returning Function
	-------------------------------------------------------------------------------
	ProjectSet (cost=0.00..0.18 rows=4 width=4)
	  ->  Result (cost=0.00..0.05 rows=1 width=0)

	EXPLAIN SELECT random();  -- не генерирующая, потому что сразу Result
	---------------------------------------------------------------------------------------
	Result  (cost=0.00..0.05 rows=1 width=8)
	
	
По сути, в этот момент происходит "запоминание" результата вычисления всего набора результирующих значений. То есть, в зависимости от того что это за функция (возвращающая единственное значение или целую выборку) и где мы ее вызвали 
(во FROM-части запроса или в качестве столбца результирующей выборки), мы можем получить 3 варианта узлов: Result, Function Scan и ProjectSet.



###################
Subquery Scan

Раз мы научились что-то из функций извлекать, давайте попробуем как-то с этим результатом повзаимодействовать. И когда мы начинаем это делать, завернув саму основную выборку во вложенный запрос, в плане возникает узел Subquery Scan:


	Subquery Scan (вложенная выборка)
	
	Обращение к значениям вложенной выборки
	EXPLAIN SELECT * FROM (SELECT random() x) T WHERE x < 0.5;
	--------------------------------------------------------------------------
	Subquery Scan on t (cost=0.00..0.11 rows=1 width=8)
	  Filter: (t.x < '0.5'::double precision)
	  -> Result (cost=0.00..0.05 rows=1 width=8)


Причем у нас действительно должно быть взаимодействие со столбцами этой выборки, иначе, например, если мы в этом примере убрать условие, от всего плана останется единственный Result, поскольку последние версии PostgreSQL достаточно 
умны для упрощения и запроса, и порожденного им плана, избавляясь от бесполезных "SELECT-от-SELECT".



###################
CTE Scan/CTE

А вот если вложенную выборку поместить не в подзапрос, а в WITH-часть, о которой говорили на третьей лекции, то появится возможность делать обращения к Common Table Expression (https://postgrespro.ru/docs/postgresql/16/queries-with), 
"запоминающих" результат выборки:

	CTE/CTE Scan (Обращение к Common Table Expression)
	
	Запрос с WITH [MATERIALIZED]
	EXPLAIN WITH T AS MATERIALIZED (VALUES (1), (2)) TABLE T;
	-----------------------------------------------------------------------
	CTE Scan on t (cost=0.11..0.31 rows=2 width=4)
	  CTE t
	    -> Values Scan on "*VALUES*" (cost=0.00..0.11 rows=2 width=4)

	EXPLAIN WITH T AS (VALUES (1), (2)) TABLE T; -- однократное
	-------------------------------------------------------------------------
	Values Scan on "*VALUES*" (cost=0.00..0.11 rows=2 width=4)


До версии PostgreSQL 12 наличие обращения к CTE однозначно приводило к появлению в плане узлов CTE Scan, где мы обращаемся к выборке, и узлов CTE, где эта выборка формируется. Но, уже начиная с v12, в случае однократного нерекурсивного 
обращения и неиспользования ключевого слова MATERIALIZED, узел CTE Scan в плане будет заменен на ее непосредственное вычисление (https://habr.com/ru/companies/postgrespro/articles/451344/) без генерации CTE.



###################
Recursive Union/WorkTable Scan

Но CTE бывают еще и рекурсивные, мы с их помощью в прошлой лекции вычисляли ряд чисел Фибоначчи. В этом случае в плане мы увидим узлы Recursive Union, который реализует добавление найденного на текущем шаге к набранному на предыдущих 
итерациях, и WorkTable Scan - итеративного обращения к предыдущему сегменту накапливаемых данных:

	Recursive Union/WorkTable Scan (Построение рекурсивной выборки)


	Запрос с WITH RECURSIVE
	      |----|-----|
	----->|  1 |  A  |  ------\   
	      |----|-----|        |
		                  |
	/-----------------------/                                 |----|------|
	|     /--    |----|------|                                |  1 |  A   |
	|     |      |  1 |  AA  |                                |----|------|
	\-->  |      |----|------| ---\                           |  1 |  AA  |
              |      |  2 |  AB  |    |                           |----|------|
	      \--    |----|------|    |                           |  2 |  AB  |
 	                              |                     =     |----|------|
        /-----------------------------/                           |  1 | AAZ  |
	|     /--   |----|--------|                               |----|------|
	|     |     |  1 |  AAZ   |                               |  2 | ABA  |
	|     |     |----|--------|                               |----|------|
	\-->  |     |  2 |  ABA   | ----\                         |  3 | ABX  |
	      |     |----|--------|     |                         |----|------|
	      |     |  3 |  ABX   |     |
	      \--   |----|--------|     |
			                |
	 <------------------------------/



	Запрос с WITH RECURSIVE (рекурсивный запрос)
	
	CTE Scan on fib (cost=13.63..16.73 rows=31 width=12)
	  CTE fib
	    -> Recursive Union (cost=0.0..13.63 rows=31 width=12)
  	      -> Result (cost=0.00..0.05 rows=1 width12)
                        -> WorkTable Scan on fib fib_1 (cost=0.00..1.05 rows=3 width=12)
	        Filter: (i < 10)




###################
InitPlan/SubPlan

Раз мы упомянули такие "ничего не делающие" узлы плана как CTE, лишь описывающие свое тело, то стоит рассказать еще про два подобных узла (в плане они выводятся "без стрелочек") - InitPlan и SubPlan, относящихся к вложенным запросам. 
Между ними лишь одно, очень тонкое, отличие. Если вложенный запрос ни от чего "снаружи" не зависит, то у вас будет узел InitPlan и его однократное выполнение:

	InitPlan (независимый вложенный запрос)
	
	Вложенный запрос, не зависящий ни от чего
	EXPLAIN SELECT (SELECT random()) * i FROM generate_series(1, 4) i;
	-------------------------------------------------------------------------------
	Function Scan on generate_series i (cost=0.06..0.26 rows=4 width=9)
	  InitPlan 1 (returns $0)
	    -> Result (cost=0.00..0.05 rows=1 width8)


В этом примере мы для каждой записи последовательности вычислили random() и умножили на значение. А в этом - умножение на значение мы сделали внутри подзапроса, и получили узел SubPlan:

	SubPlan (зависящий от внешнего вложенный запрос)

	Вложенный запрос, зависящий от других
	EXPLAIN SELECT (SELECT random() * i) FROM generate_series(1, 4) i;
	----------------------------------------------------------------------------------
	Function Scan on generate_series i (cost=0.00..0.43 rows=4 width=9)
	  SubPlan 1
	    -> Result (cost=0.00..0.06 rows=1 width=8)

Мало того, что мы получаем разные планы - так еще и результаты принципиально различны! В случае с InitPlan random() выполняется лишь раз, и все значения пропорциональны первому, то в варианте с SubPlan, вычисление функции происходит 
каждый раз:

	Разница InitPlan - лишь раз, а SubPlan - каждый раз
	
	EXPLAIN (ANALYZE, ...) SELECT (SELECT random()) * i FROM generate_series(1, 4) i;
	-------------------------------------------------------------------------------------------------
	Function Scan on generate_series i (actual time=0.010..0.011 rows=4 loops=1)
	  InitPlan 1 (returns $0)
	    -> Result (actual time=0.001..0.001 rows=1 loops=1)


	EXPLAIN (ANALYZE, ...) SELECT (SELECT random() * i) FROM generate_series(1, 4) i;
	-------------------------------------------------------------------------------------------------
	Function Scan on generate_series i (actual time=0.009..0.012 rows=4 loops=1)
	  SubPlan 1
 	    -> Result (actual time=0.000..0.001 rows=1 loops=4) -- по каждой записи



	Разница InitPlan - все значения пропорциональны первому, а SubPlan - все независимы


	SELECT (SELECT random()) * i FROM generate_series(1, 4) i;
	-----------------------------------------------------------------------
	0.100151624911382235 	-- x1
	0.2003032498276447	-- x2
	0.30045487474146704	-- x3
	0.4006064996552894	-- x4 - размножилось ОДНО значение

	
	SELECT (SELECT random() * i) FROM generate_series(1, 4) i;
	-----------------------------------------------------------------------
	0.6490647188923726
	0.6605839339960129
	0.5961901827957088	-- ничего общего
	1.476483188079107	-- все значения вычислены независимо


Этот момент, безусловно, стоит учитывать, когда вы пишете SQL с вложенным запросом - сколько раз "на самом деле" он у вас выполняется.

	


______________________________________________________________________________________
Чтение из таблиц

Наконец-то мы закончили с узлами, получающими данные "из воздуха", без обращения к каким-то "физическим" контейнерам, и дальше уже будем смотреть, как PostgreSQL умеет извлекать данные из файлов таблиц/индексов. Подробно этого вопроса я 
касался в статье о поиске избыточно занятого места (https://habr.com/ru/post/542058/), а если вкратце, то таблица/индекс хранится в одном контейнере, который "физически" делится на файлы-сегменты по 1GB, которые, в свою очередь, на 
логические страницы данных по 8KB:

	Модель физического хранения данных таблицы
	
	Файл таблицы/индекса разделён  на сегменты по 1GB
	  каждый сегмент - на страницы по 8 KB

	                       1GB					                  1GB                                                             < 1GB
	|---------------------------------------------------|        |---------------------------------------------------|        |---------------------------------------------------|
	|                     12345                         | ====>  |                   12345.1                         | ====>  |                    12345.2                        | 
	|---------------------------------------------------|        |---------------------------------------------------|        |---------------------------------------------------|
	
	    8KB                                                             8KB
	|---------|---------|---------|     |---------------|        |---------------|
	| page 0  | page 1  | page 2  | ... | page 131'071  |        | page 131'072  |  ...
	|---------|---------|---------|     |---------------|        |---------------|


Чтение каждой страницы - это и есть "+1" в соответствующее значение buffers, о которых мы говорили в первой части лекции - в зависимости от того, была ли она уже спроецирована куда-то в память, или нам приходится читать ее с носителя. На каждой такой 
странице отдельно лежат указатели на начало каждой из записей и отдельно их двоичные представления:


	Модель хранения записей на странице

	Каждая страница содержит список кортежей

		             Порядок чтения
                               ----------------------------------------------->
                             
			         блок указателей
		               /-----------------------------------------\
                               |                                         |
	/---       |-------------|-----------|-----------|-----------|---|---------------------------------------------|
	|          | заголовок   | ItemId 1  | ItemId 2  | ItemId 3  |   |                                             |
        |          |-------------|-----------|-----------|-----------|   |                                             |
	|          |           |                                         |                                             |
        |          |           \-----------------------------------------/                                             |
        |          |                                                                                                   |
   8KB  |          |                                                            блок записей                           |
        |          |        /------------------------------------------------------------------------\                 |
        |          |        |                                                                        |                 |
        |          |        |   |-----------------|------------------------------------|----------|--------------------|
        |          |        |   |                 |                                    |          |                    |
        |          |        |   |       Item 3    |                    Item 2          |  Item 1  | "особый" раздел    |
        \--        |--------|---|-----------------|------------------------------------|----------|--------------------|
                            |                                                                        |
                            \------------------------------------------------------------------------/




###################
TID Scan

Поэтому самый быстрый и простой способ обращения к одной конкретной записи - прямая адресация, представленная узлом TID Scan - буквально, "прочитай со страницы #2 запись #3".

	TID Scan (Чтение записи по "физическому" расположению (pageNo, tupleNo))

	Чтение конкретной записи по физическому адресу
	EXPLAIN SELECT * FROM pg_class WHERE ctid = '(2,3)';
	----------------------------------------------------------------
	Tid Scan on pg_class (cost=0.00..1.15) rows=1 width=265)
	  TID Cond: (ctid = '(2,3)'::tid)






		|--------|--------|--------|---------|    
		| page 0 | page 1 | page 2 | page 3  |  ...
		|--------|--------|--------|---------|   
				      ||
				      ||
				      ||
                  /------------------/  \-----------------------------------------------------------------------------\   
	          |  								                                      |
	          |								                                      |
	          |-------------|-----------|-----------|-----------|-------------------------------------------------|
		  | заголовок   | ItemId 1  | ItemId 2  | ItemId 3  |                                                 |
            	  |-------------|-----------|-----------|-----------|                                                 |
		  |                                           ||                                                      |
                  |                                           ||                                                      |
                  |                      //===================//                                                      |
                  |                      ||                                                                           |
                  |                      ||                                                                           | 
                  |                      \/                                                                           |
                  |          |-----------------|------------------------------------|----------|----------------------|
                  |          |                 |                                    |          |                      |
                  |          |      Item 3     |                    Item 2          |  Item 1  | "особый" раздел      |
                  |----------|-----------------|------------------------------------|----------|----------------------|



Его можно использовать для такой вещи как идентификация строк в таблице (https://habr.com/ru/post/481352/), где уникальные ключи в принципе отсутствуют (типа таблиц логов или значений метрик мониторинга), но поработать с конкретными 
экземплярами записей хочется - например, чтобы удалить лишние дубли.
                           
		Зачем бывает нужен TID Scan?
		Чтение конкретной записи по физическому адресу
		идентификация записей таблицы без Primary Key
		"цепочка" запросов по одним и тем же строкам
		например, удаление дублей строк из таблицы



###################
Seq Scan

Следующий, самый простой и, наверное, наиболее часто встречающийся узел - Seq Scan - последовательный просмотр страниц таблицы. То есть "указатель" чтения встает в начало файла таблицы, или может, и в середину - как базе покажется 
"удобнее" - и начинает читать страницу за страницей:

	Seq Scan (читаем всё подряд)

	Последовательный просмотр страниц и их записей
	EXPLAIN SELECT * FROM  pg_class;	
	------------------------------------------
	Seq Scan on pg_class (cost=0.0..34.90 rows=418 width=265)



		====================================>
		
		|------------|------------|------------|-------------|    
		| page N + 0 | page N + 1 | page N + 2 | page N + 3  |  ...
		|------------|------------|------------|-------------|   
				                 ||
				                 ||
				                 ||
                  /-----------------------------/  \------------------------------------------------------------------\   
		  |  								                                      |
		  |	  =====================>		                                                      |
		  |-------------|-----------|-----------|-----------|-------------------------------------------------|
		  | заголовок   | ItemId 1  | ItemId 2  | ItemId 3  |                                                 |
            	  |-------------|-----------|-----------|-----------|                                                 |
                  |								                                      |
	          |								                                      |
                  |          |-----------------|------------------------------------|----------|----------------------|
                  |          |                 |                                    |          |                      |
                  |          |       Item 3    |              Item 2                |  Item 1  | "особый" раздел      |
                  |----------|-----------------|------------------------------------|----------|----------------------|


Этот способ не только не требует какой-то дополнительной настройки и доступен для любой таблицы всегда, но и наиболее быстр как при чтении с любого носителя, так и при доступе к памяти - поскольку последовательный доступ получает все 
преимущества упреждающего кэширования. Если вы читаете из небольшой таблицы, то никакой другой вариант доступа быстрее заведомо не будет. Если у вас в самой таблице 3 страницы, да еще и в индексе те же 3 - какой смысл читать 
тогда плюсом индекс?..


	Seq Scan (самый быстрый, доступен всегда)
	Последовательный просмотр страниц и их записей
	самый простой, а потому самый быстрый способ      (Оффтоп от себя: мне кажется или последовательный перебор всех записей это не самый быстрый способ?)
	оптимален для небольших таблиц (сотни строк)
	... или когда иначе никак (нет нужных индексов)


Или если у вас на таблице нет никаких подходящих к условиям запроса индексов - Seq Scan выручит всегда: надо - пофильтруем, надо - потом отсортируем. Но если у вас в плане Seq Scan возвращает или фильтрует много-много 
записей (rows/RRbF) - это неудачный план, и такой подход становится объективным злом.



###################
Index Scan [Backward]

Чтобы это зло лишний раз не тревожить, люди придумали индексы. Ведь понятно, что "самый быстрый" не означает "самый эффективный" - Seq Scan прочитает быстро-быстро, но все-все записи из всех страниц, которые ему попадутся, и, наверняка,
большинство из них отбросит как неподходящие по условиям. Индекс - это дополнительная, как правило, древовидная логически упорядоченная структура, которая хранится аналогично модели хранения таблицы. В индексе мы всегда знаем, к 
какой его следующей странице надо двигаться, чтобы дойти до конкретной записи, соответствующей условиям в запросе - за это "движение" как раз и отвечает узел Index Scan - чтение записей таблицы в порядке, заданном индексом:

	Index Scan [Backward] (упорядоченный доступ к записям)
		
	Индексированный поиск в таблице
	EXPLAIN SELECT * FROM pg_class WHERE relname = 'pg_class';
	------------------------------------------------------------------------------
	Index Scan using pg_class_relname_nsp_index on pg_class (cost=0.27..2.53 rows=1 width=265)
	                 --------------------------    --------
		                    имя индекса         имя таблицы

	  Index Cond: (relname = 'pg_class'::name) 	-- Index Cond - ключ индекс поиска


Однако, упорядоченным этот доступ может быть только лишь по тем ключам и условиям (https://habr.com/ru/post/488104/), которые были заранее заданы при создании индекса. В этом случае в атрибутной строке Index Cond мы увидим как раз то 
условие, которое используется для поиска записей. Если такие условия обратимы с точки зрения линейного порядка ("справа -налево" вместо "слева - направо") следования записей в индексе (например, |x > y| и |x <= y|), то при "обратном" 
чтении узел будет Index Scan Backward. Но проблема в том, что такие "случайные" переходы "по дереву" между страницами индекса, а потом еще и обращение к разным участкам файла таблицы, не слишком эффективны с точки зрения random read, 
поэтому читать лучше как можно меньше записей:

	Index Scan [Backward] (быстро, если читать немного)
	Индексированный поиск в таблице
	подходящий к условиям индекс PostgreSQL подбирает сам
	спускаемся по "дерево" индекс, затем идём в страницу данных
	не стоит читать много-много записей(random read!)



###################
Index Only Scan [Backward]

Но если запись индекса и так "знает", что "вот там лежит запись со значением ключевого поля N", то саму запись таблицы можно уже и не извлекать, если кроме самого значения ключевого поля возвращать ничего не требуется - в этом случае 
в плане окажется узел Index Only Scan. Тут важно помнить, что использование SELECT * в запросе почти всегда сделает использование Index Only Scan невозможным.

Основное отличие от Index Scan заключается в необходимости убедиться, что записи на странице, куда указывает индекс, "видны всем" - то есть доступны всем активным транзакциям в одинаковом состоянии. Это делается с помощью проверки 
всего лишь пары битов Visibility Map (https://postgrespro.ru/docs/postgresql/16/storage-vm), что гораздо "дешевле" извлечения записей таблицы:

	Index Only Scan [Backward] (пробуем не заглядывать в таблицу)
	
	Чтение из индекса, без обращения к таблице
	EXPLAIN SELECT relname FROM pg_class WHERE relname = 'pg_class';
	-------------------------------------------------------------------------------------
	Index Only Scan using pg_class_relname_nsp_index on pg_class (cost=0.27..1.43 rows=1 width=64)
	  Index Cond: (relname = 'pg_class'::name)
	

Причем, мы можем "дешево" вернуть из записи индекса значения не только ключевых полей, по которым можно искать, но и дополнительно "прицепленных" к ним директивой INCLUDE: 

	Index Only Scan [Backward] (добавление неключевых полей)
	Чтение из индекса, без обращения к таблице
	получение только ключевых/включенных полей индекса
	CREATE INDEX ON tbl(a, b, c) INCLUDE (x, y, z)



###################
Heap Fetches

Но даже если мы создали подходящий индекс, в запросе написали только подходящие поля, в плане получили Index Only Scan - это вовсе не гарантия, что наш запрос выполнится быстро. Если вдруг мы видим ненулевое значение в атрибутной строке 
Heap Fetches - значит, где-то VM оказалась в неактуальном состоянии, и нам уже после этой проверки пришлось дополнительно "лезть" в таблицу. В этом случае "эффективный" Index Only Scan может оказаться не просто "не лучше", а гораздо 
хуже (https://habr.com/ru/post/751458/), чем "обычный" Index Scan:

	Heap Fetches (обрабатываем "промах" сквозь VM)

	Строк получено из таблицы при IOS 
	когда их много, IOS проигрывает "обычному" IS
	EXPLAIN (ANALYZE, ...) SELECT relname FROM pg_class WHERE relname = 'pg_class';
	--------------------------------------------------------------------------------------------------
	Index Only Scan using pg_class_relname_nsp_index on pg_class (actual time=0.014..0.014 rows=1 loops=1)
	  Index Cond: (relname = 'pg_class'::name)
	  Heap Fetches: 1



###################
Order By

Еще одна атрибутная строка, которую можно увидеть у узла Index Only Scan для некоторых типов индексов, - условие индексного упорядочивания Order By:

	Order By (доступно для некоторых индексов и операций)
	Индексное упорядочивание(<->, k-NN search)
	если его поддерживает тип индекса и класс оператора
	GiST, SP-Gist

Как правило, она возникает при использовании оператора <-> для поиска "ближайших соседей" в различных ГИС-системах:

	Order By (близжайшие 10 точек из миллиона)
	
	Индексное упорядочивание(k-NN search)
	CREATE TABLE places AS
	  SELECT 
	    point(random(), random()) p
	  FROM
	    generate_series(1, 1e6) i; -- миллион случайных точек [0..1, 0..1]
	CREATE INDEX ON places USING gist(p);	--GiST-индекс
	EXPLAIN (ANALYZE, ...) SELECT * FROM places ORDER BY p <-> '(0.5,0.5)'::point LIMIT 10;
	----------------------------------------------------------------------------------------------------------
	Limit (actual time=0.324..0.333 rows=10 loops=1)
	  -> Index Only Scan using places_p_inx on places (actual time=0.323..0.331 rows=10 loops=1)
	    Order By: (p <-> '(0.5,0.5)'::point)
	    Heap Fetches: 0



______________________________________________________________________________________
   "Нетипичное" получение данных

Дальше пойдут гораздо более редко встречающиеся "в дикой природе" варианты получающих данные узлов.


###################
Sample Scan

Если вам надо прочитать из огромной таблицы "хоть что-то", примерно демонстрирующее распределение данных в ней - вам поможет конструкция TABLESAMPLE, которая порождает узел Sample Scan, атрибутная строка Sampling которого как раз и 
демонстрирует выбранный принцип отбора записей:

	Sample Scan (читаем "кусочек" таблицы)
	
	Чтение "примера" из таблицы
	EXPLAIN SELECT * FROM pg_class TABLESAMPLE BERNOULLI(1);
	----------------------------------------------------------------------------
	Sample Scan on pg_class (cost=0.00..14.20 rows=4 width=265)
	  Sampling: bernoulli ('1'::real)   -- Sampling - правила отбора записей


Фактически, TABLESAMPLE - это более эффективный вариант чтения записей по случайному вероятностному условию:

	Sample Scan
	Чтение "примера" из таблицы
	быстрый аналог WHERE random() < X
	
	TABLESAMPLE метод_выборки (аргумент [, ...] ) [ REPEATABLE ( затравка ) ]
	"Методы выборки BERNOULLI и SYSTEM принимают единственный аргумент, определяющий, какой процент таблицы должен попасть в выборку, от 0 до 100. Этот аргумент может задаваться любым выражением со значением типа real. (Другие
	методы выборки могут принимать дополнительные или другие параметры.) Оба этих метода возвращают случайную выборку таблицы, содержащую примерно указанный процент строк таблицы. Метод BERNOULLI сканирует всю таблицу и выбирает 
	или игнорирует отдельные строки независимо, с заданной вероятностью. Методу SYSTEM строит выборку на уровне блоков, определяя для каждого блока шанс его задействовать, и возвращает все строки из каждого задействуемого блока".




###################
Table Function Scan

Вызов вот так нетрадиционно описываемой SQL-стандартом функции XMLTABLE (и пока вроде только для нее) порождает в плане узел Table Function Scan:

	Table Function Scan (редкий гость)
	
	Выполнение функции XMLTABLE
	EXPLAIN SELECT * FROM XMLTABLE('\' PASSING '<xml/>' COLUMNS id integer)
	-----------------------------------------------------------------------------------------------
	Table Function Scan on "xmltable"   (cost=0.00..5.00 rows=100 width=4)




###################
Foreign Scan

PostgreSQL имеет весьма обширную экосистему, поэтому имеет возможности для работы и со сторонними СУБД - при обращении к таким "внешним" БД в плане появляется узел Foreign Scan с атрибутной строкой Remote SQL, в которой можно увидеть
реальный запрос, ушедший "наружу":

	Foreign Scan (чтение из "сторонней" БД)
	
	Обращение к стороннему серверу
	EXPLAIN VERBOSE SELECT * FROM foreign_pg_class;
	---------------------------------------------------------------------
	Foreign Scan on public.foreign_pg_class    (cost=100.00.207.90 rows=890 width=68)
	  Output: oid, relname
	  Remote SQL: SELECT uid, relname FROM pg_catalog.pg_class      -- Remote SQL - запрос к целевому серверу


Например, для распределения нагрузки между физическими серверами мы можем использовать основной просто как "прокси", транслирующий запросы к разным таблицам на независимые узлы.

Чтобы иметь возможность работать с таблицей из "внешней" базы, нам необходимо...

1. Описать реквизиты самой базы (имена сервера и БД, логин-пароль)

	Foreign Scan (подключение "внешнего" сервера PostgreSQL)
	
	Обращение к стороннему серверу
	масштабирование и распределение нагрузки
	
	CREATE EXTENSION postgres_fdw; -- устанавливаем расширение для доступа у внешнему PostgreSQL-серверу
	CREATE SERVER foreign_server
	  FOREIGN DATA WRAPPER postgres_fdw
	  OPTIONS (host 'localhost', dbname '_test');  -- определяем координаты внешнего сервера
	CREATE USER MAPPING FOR PUBLIC
	  SERVER foreign_server
	  OPTIONS (password 'postgres');  -- пробрасываеся доступ пользователям



2. Описать отображение внешней таблицы

	Foreign Scan (подключение "внешней" таблицы)
	Обращение к стороннему серверу
	масштабирование и распределение нагрузки

	CREATE FOREIGN TABLE foreign_pg_class (
	  oid oid
	, relname name
	) -- имя и формат "локальной" прокси таблицы
	  SERVER foreign_server
	  OPTIONS (
	    schema_name 'pg_catalog'
	  , table_name 'pb_class'
	  ); -- схема и имя таблицы на внешнем сервере



3. Выполнить запрос к локальным отображниям

	Foreign Scan (соединение таблиц на внешнем сервере)
	Обращение к стороннему серверу
	перенос соединений (JOIN) на внешний сервер
	
	EXPLAIN SELECT * FROM
	  foreign_pg_class c JOIN foreign_pg_index i
	    ON i.indexrelid = c.oid
	LIMIT 1;
	------------------------------------------------------
	Foreign Scan (cost=100.00..100.55 rows=1 width=76)
	  Relations: (public.foreign_pg_class c) INNER JOIN (public.foreign_pg_index i)  -- Relations - соединение на внешнем сервере


В случае соединения в запросе сразу нескольких таблиц, находящихся в одной внешней БД, PostgreSQL может передать реализацию соединения самому внешнему серверу - в этом случае в узле Foreign Scan появится атрибутная строка Relations,
показывающая это.



###################
Async Foregn Scan

Но передать на подчиненные серверы только извлечение или соединение данных - мало. Хочется еще и уметь получать с них данные одновременно - это реализует узел Async Foreign Scan:

	Async Foregn Scan (одновременное получение "внешних" данных)
	Обращение к стороннему серверу

	EXPLAIN SELECT * FROM proxy_pg_class;
	-----------------------------------------------------
	Append (cost=0.00..460.33 rows=1781 width=68)
	  -> Seq Scan on proxy_pg_class proxy_pg_class 1 (cost=0.00..0.00 rows=1 width=68)
	  -> Async Foreign Scan on foreign_1_pg_class proxy_pg_class_2 (cost=100.00..207.90 rows=890 width=68)
	  -> Async Foreign Scan on foreign_2_pg_class proxy_pg_class_3 (cost=100.00..207.90 rows=890 width=68)


Но сделать он это сможет, только если при описании сервера указан ключ async_capable (https://postgrespro.ru/docs/postgresql/16/postgres-fdw#POSTGRES-FDW-OPTIONS-ASYNCHRONOUS-EXECUTION):

	Async Foregn Scan (ключ async_capable разрешает асинхронное выполнение)
	Обращение к стороннему серверу
	
	CREATE TABLE proxy_pg_class(oid oid, relname name);			-- имя и формат локальной таблицы
	CREATE SERVER foreign_server_1
	  FOREIGN DATA WRAPPER postgres_fdw
	  OPTIONS (host 'localhost', dbname '_test', async_capable 'true');	-- включаем асинхронность
	CREATE USER MAPPING FOR PUBKIC
	  SERVER foreign_server_1
	  OPTIONS (password 'postgres');
	CREATE FOREIGN TABLE foreign_1_pg_class()
	  INHERITS (proxy_pg_class)						-- наследуемся от локальной таблицы
	  SERVER foreign_server_1
	  OPTIONS (schema_name 'pg_catalog', table_name 'pg_class');
	-- повторить для foreign_server_2 / foreign_2_pg_class


	
	Async Foregn Scan (проброс запроса)
	EXPLAIN (ANALYZE, VERBOSE, ...) SELECT * FROM proxy_pg_class LIMIT 1;
	--------------------------------------------------------------------------------------
	Limit (actual time=4.851..4.853 rows=1 loops=1)
	  Output: proxy_pg_class.oid, proxy_pg_class.relname
	  -> Append (actual time=4.849..4.851 rows=1 loops=1)
	    -> Seq Scan on public.proxy_pg_class proxy_pg_class_1 (actual time=0.004..0.004 rows=0 loops=1)
	      Output: proxy_pg_class_1.oid, proxy_pg_class_1.relname
	    -> Async Foreign Scan on public.foreign_1_pg_class proxy_pg_class_2 (actual time=4.086..4.087 rows=1 loops=1)
	      Output: proxy_pg_class_2.oid, proxy_pg_class_2.relname
	      Remote SQL: SELECT oid, relname FROM pf_catalog.pg_class
	    -> Async Foreign Scan on public.foreign_2_pg_class proxy_pg_class_3 (actual time=0.750..0.750 rows=0 loops=1)
	      Output: proxy_pg_class_3.oid, proxy_pg_class_3.relname
	      Remote SQL: SELECT oid, relname FROM pf_catalog.pg_class



###################
Named Tuplestore Scan

Крайне редкий узел Named Tuplestore Scan появляется в плане, когда мы внутри STATEMENT-триггера что-то делаем с NEW/OLD-проекциями:

	Named Tuplestore Scan (работа с NEW/OLD в триггере)
	
	Обращение к записям внутри триггера
	NEW/OLD внутри STATEMENT-триггера

	RESULT (actual time=0.007..0.007 rows=1 loops=1)
	  InitPlan 1 (returns $0)
	    -> Aggregate (actual time=0.004..0.005 rows=1 loops=1)
	      -> Named Tuplestore Scan (actual time=0.001..0.001 rows=2 loops=1)


В качестве примера попробуем написать триггер, который просто будет выводить NOTICE с количеством вставляемых в таблицу записей:

	Named Tuplestore Scan (Считаем кол-во вставляемых записей)

	Обращение к записям внутри триггера
	CREATE TABLE tbl(i integer);

	CREATE OR REPLACE FUNCTION count_trigger() RETURNS trigger AS $$
	BEGIN
	  RAISE NOTICE 'count = %', (SELECT count (*) FROM new_ref);  -- вывели в консоль кол-во вставляемых записей
	  RETURN NULL;
	END
	$$ LANGUAGE plpgsql;

	CREATE TRIGGER tbl_trigger AFTER INSERT ON tbl
	  REFERENCING NEW TABLE AS new_ref 	-- "пробросили" NEW внутрь триггера под именем new_ref
	  FOR EACH STATEMENT		--  только для STATEMENT-триггеров
	    EXECUTE PROCEDURE count_trigger();

Теперь, если настроить максимально полное протоколирование всех планов в текущей сессии, и выполнить вставку в таблицу...

	Named Tuplestore Scan (Выкручиваем логирование планов "на максимум")
	
	Обращение к записям внутри триггера
	Перехватываем планы внутри триггеров

	LOAD 'auto_explain';
	SET auto_explain.log_analyze = 'on';
	SET auto_explain.log_buffers = 'on';
	SET auto_explain.log_timing = 'on';
	SET auto_explain.log_min_duration = 0;		-- снимаем планы вообще всех запросов
	SET auto_explain.log_nested_statements = 'on'; 	-- ... и вложенных - тоже
	SET auto_explain.log_triggers = 'on';


	Выполняем втсавку - в логах видим план "изнутри" триггера
	INSERT INTO tbl VALUES (1), (2);
	---------------------------------------
	NOTICE: count = 2
	Query returned successfully: 2 rows affected, 13 msec execution time.
	Result (cost=0.26..0.30 rows=1 width=8) (actual time=0.007..0.007 rows=1 loops=1)
	  InitPlan 1 (returns $0)
	    -> Aggregate (cost=0.21..0.26 rows=1 width=8) (actual time=0.004..0.005 rows=1 loops=1)
	      -> Named Tuplestore Scan (cost=0.00..0.20 rows=2 width=0) (actual time=0.001..0.001 rows=2 loops=1)




###################
Custom Scan

Начиная с 11-12 версии PostgreSQL вы можете описать собственные методы доступа к файлам данных, используя стандартное API. Фактически, вы можете хранить и читать данные так, как вам хочется - за это отвечает узел 
Custom Scan, в скобочках которого будет как раз конкретный используемый для работы с таблицей метод-провайдер:

	Custom Scan (читаю как хочу)
	Пользовательский метод доступа к данным

	Custom Scan (HypertableModify)
	  -> Insert on _materialized_hypertable_15		-- HypertableModify и ChunkDispatch - пользовательские методы-провайдеры
	    -> Custom Scan (ChunkDispatch)
	      ...


Такие планы обычно возникают при использовании различных расширений или "форков" - вроде Citus или TimescaleDB, иногда Greenplum. То есть, если вы хороший C-разработчик, то можете сделать и свой метод доступа.



______________________________________________________________________________________
Работа с битовыми картами

Битовые карты позволяют совместить преимущества скорости Seq Scan и избирательности Index Scan, но, в отличие от всех предыдущих вариантов получения данных, работа с ними уже требует наличия в плане нескольких 
узлов.


###################
Bitmap: Index Scan | And/Or | Heap Scan

Сначала в узле Bitmap Index Scan база "отмечает" те страницы данных, в которых искомые значения могли бы находиться, согласно использованному индексу. Затем, если использовалось несколько индексов, над полученными 
из них "картами" производится операция пересечения/объединения BitmapAnd/BitmapOr в зависимости от операции между условиями. После этого в узле Bitmap Heap Scan отмеченные в "карте" страницы прочитываются и 
на них ищутся записи, подходящие под полную комбинацию условий:

	Работа с битовыми картами
	Последовательный просмотр страниц по маске
	можно комбинировать несколько индексов/условий



###################
Recheck Cond

Вся такая комбинация проверяемых условий как раз и выводится в строке Recheck Cond:

	Работа с битовыми картами
	Последовательный просмотр страниц по маске
	EXPLAIN SELECT * FROM pg_class WHERE relname = 'pg_class' OR relname = 'pg_class';
	-----------------------------------------------------------------------------------------------------
	Bitmap Heap Scan on pg_class (cost=2.76..6.00 rows=2 width=265)
	  Recheck Cond: ((relname = 'pg_class'::name) OR (relname = 'pg_index'::name))			-- Recheck Cond - перепроверка индексного условия
	  -> BitmapOr (cost=2.76..2.76 rows=2 width=0)
	    -> Bitmap Index Scan on pg_class_relname_nsp_index (cost=0.00..1.38 rows=1 width=0)
	      Index Cond: (relname = 'pg_class'::name)
	    -> Bitmap Index Scan on pg_class_relname_nsp_index (cost=0.00..1.38 rows=1 width=0)
	      Index Cond: (relname = 'pg_index'::name)



###################
Rows Removed by Index Recheck

Поскольку сами карты имеют вероятностный характер, отражая лишь потенциальную возможность наличия подходящих записей, количество всех прочитанных, но не соответствующих полной комбинации условий, будет 
выведено в строке Rows Removed by Index Recheck:

	Rows Removed by Index Recheck (кол-во отфильтрованных строк)
	Кол-во отброшенных перепроверкой записей
	
	Bitmap Heap Scan on bitmap_tst (actual rows=20107 loops=1)
	  Recheck Cond: ((random < '0.01'::double precision) OR (random > '0.99'::double precision))
	  Rows Removed by Index Recheck: 801273
	  Heap Blocks: exact=752 lossy=3624
	  -> BitmapOr (actual rows=0 loops=1)
	    -> Bitmap Index Scan on bitmap_tst_random_idx (actual rows=9989 loops=1)
	      Index Cond: (random < '0.01'::double precision)
	    -> Bitmap Index Scan on bitmap_tst_random_idx (actual rows=10118 loops=1)
	      Index Cond: (random < '0.99'::double precision)
	


###################
Heap Blocks

При построении карты PostgreSQL сначала пытается сохранить "координаты" подходящих записей (exact), но если даже сама "карта" оказывается настолько велика, что не умещается в доступной памяти, то там остаются лишь 
номера страниц (lossy):

	Heap Blocks (влезла ли "карта")
	Количество прочитанных из таблицы блоков
	
	Bitmap Heap Scan on bitmap_tst (actual rows=20107 loops=1)
	  ...
	  Heap Blocks: exact=752 lossy=3624
	  ...

	-- exact - в карте записан адрес до конкретной записи (page, tuple)
	-- lossy - не хватило work_mem, поэтому в карте только (page)
	-- каждую запись страницы приходится вычитывать и перепроверять


Понятно, что читать все записи со страницы при этом гораздо "дороже", чем проверить только конкретные. Если видите такую ситуацию в строке Heap Blocks, можете попробовать увеличить значение параметра work_mem
(https://postgrespro.ru/docs/postgresql/16/runtime-config-resource#GUC-WORK-MEM).


На этом лекция закончена, и дальше будет рассмотрение других видов операций над уже полученными данными.
